{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning with Tensorflow\n",
    "=============\n",
    "\n",
    "Assignment II\n",
    "------------\n",
    "\n",
    "During one of the lectures in [Lab 1](https://deep-learning-su.github.io/labs/lab-1/) we trained fully connected network to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters. \n",
    "\n",
    "The goal of this assignment is make the neural network convolutional.\n",
    "\n",
    "For this exercise, you would need the `notMNIST.pickle` created in `Lab 1`. You can obtain it by rerunning the given paragraphs without having to solve the problems (although it is highly recommended to do it if you haven't already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "y3-cj1bpmuxc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "IRSyYiIIGIzS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def output_size_conv(in_size, filter_size, padding, stride):\n",
    "    return int(np.ceil((in_size - filter_size + 2 * padding) / stride) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Size checks\n",
    "out1 = output_size_conv(image_size, 5, 1, 2)\n",
    "out2 = output_size_conv(out1, 5, 1, 2)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "## Problem 1\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n",
    "\n",
    "Edit the snippet bellow by changing the `model` function.\n",
    "\n",
    "### 1.1 - Define the model\n",
    "Implement the `model` function bellow. Take a look at the following TF functions:\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "### 1.2 - Compute loss\n",
    "\n",
    "Implement the `compute_loss` function below. You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment2/.assign2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-60487f47d87d>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5 # Filter size 5x5?\n",
    "depth = 16 # Number of filters?\n",
    "num_hidden = 64 # Size of the fully connected layer?\n",
    "\n",
    "padding = 1\n",
    "stride = 2\n",
    "stddev = 1e-1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # image size 28\n",
    "    # ceil((28 - 5 + 2)/2 + 1) -> 14\n",
    "    \n",
    "    out1 = output_size_conv(image_size, patch_size, padding, stride)\n",
    "    size = output_size_conv(out1, patch_size, padding, stride)\n",
    "    \n",
    "    weights = {\n",
    "        'c1': tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = stddev)),\n",
    "        'c2': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = stddev)),\n",
    "        'h1': tf.Variable(tf.truncated_normal([size * size * depth, num_hidden], stddev = stddev)),\n",
    "        'o': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = stddev))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([depth])),\n",
    "        'bc2': tf.Variable(tf.zeros([depth])),\n",
    "        'bh1': tf.Variable(tf.zeros([num_hidden])),\n",
    "        'bo': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        # define a simple network with \n",
    "        # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n",
    "        # * one fully connected layer\n",
    "        # return the logits (last layer)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        conv1 = tf.nn.conv2d(data, weights[\"c1\"], [1, stride, stride, 1], padding = \"SAME\")\n",
    "        hidden = tf.nn.relu(conv1 + biases[\"bc1\"])\n",
    "        \n",
    "        # Conv layer 2\n",
    "        conv2 = tf.nn.conv2d(hidden, weights[\"c2\"], [1, 2, 2, 1], padding = \"SAME\")\n",
    "        hidden2 = tf.nn.relu(conv2 + biases[\"bc2\"])\n",
    "        \n",
    "        # Fully connected layer 3\n",
    "        # Flatten the input data\n",
    "        # We receive 16x7x7x16 -> 16x7*7*16; 16 is the batch size - can vary\n",
    "        reshapedh = tf.reshape(hidden2, (-1, size * size * depth))\n",
    "        fullc3 = tf.matmul(reshapedh, weights[\"h1\"]) + biases[\"bh1\"]\n",
    "        hidden3 = tf.nn.relu(fullc3)\n",
    "        \n",
    "        # Last layer\n",
    "        logits = tf.matmul(hidden3, weights[\"o\"]) +  biases[\"bo\"]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def compute_loss(labels, logits):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = compute_loss(tf_train_labels, logits)\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkzpbHET-m8S"
   },
   "source": [
    "### 1.3 - Measure the accuracy and tune your model\n",
    "\n",
    "Run the snippet bellow to measure the accuracy of your model. Try to achieve a test accuracy of around 80%. Iterate on the filters size. Filter 5x5x1 is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "noKFb2UovVFR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.270723\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 1000: 0.748431\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 0.176370\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 0.037978\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4000: 0.450723\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.632222\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5 # Filter size 5x5?\n",
    "depth = 16 # Number of filters?\n",
    "num_hidden = 64 # Size of the fully connected layer?\n",
    "stddev = 1e-1\n",
    "\n",
    "padding = 1\n",
    "stride = 1\n",
    "\n",
    "pool_filter_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # image size 28\n",
    "    out1 = output_size_conv(image_size, patch_size, padding, stride)\n",
    "    out1_pool = output_size_conv(out1, pool_filter_size, padding, pool_stride)\n",
    "    size = output_size_conv(out1_pool, patch_size, padding, stride)\n",
    "    size_pool = output_size_conv(size, pool_filter_size, padding, pool_stride)\n",
    "    \n",
    "    weights = {\n",
    "        'c1': tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = stddev)),\n",
    "        'c2': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = stddev)),\n",
    "        'h1': tf.Variable(tf.truncated_normal([size_pool * size_pool * depth, num_hidden], stddev = stddev)),\n",
    "        'o': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = stddev))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([depth])),\n",
    "        'bc2': tf.Variable(tf.zeros([depth])),\n",
    "        'bh1': tf.Variable(tf.zeros([num_hidden])),\n",
    "        'bo': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        \n",
    "        # Conv layer 1\n",
    "        conv1 = tf.nn.conv2d(data, weights[\"c1\"], [1, stride, stride, 1], padding = \"SAME\")\n",
    "        hidden = tf.nn.relu(conv1 + biases[\"bc1\"])\n",
    "        hidden = tf.nn.max_pool(hidden, [1, pool_filter_size, pool_filter_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], padding = \"SAME\")\n",
    "        \n",
    "        # Conv layer 2\n",
    "        conv2 = tf.nn.conv2d(hidden, weights[\"c2\"], [1, 1, 1, 1], padding = \"SAME\")\n",
    "        hidden2 = tf.nn.relu(conv2 + biases[\"bc2\"])\n",
    "        hidden2 = tf.nn.max_pool(hidden2, [1, pool_filter_size, pool_filter_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], padding = \"SAME\")\n",
    "        \n",
    "        # Fully connected layer 3\n",
    "        # Flat the input data\n",
    "        reshapedh = tf.reshape(hidden2, (-1, size_pool * size_pool * depth))\n",
    "        fullc3 = tf.matmul(reshapedh, weights[\"h1\"]) + biases[\"bh1\"]\n",
    "        hidden3 = tf.nn.relu(fullc3)\n",
    "        \n",
    "        # Last layer\n",
    "        logits = tf.matmul(hidden3, weights[\"o\"]) +  biases[\"bo\"]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def compute_loss(labels, logits):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = compute_loss(tf_train_labels, logits)\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.462160\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 9.5%\n",
      "Minibatch loss at step 1000: 0.533005\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2000: 0.083180\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3000: 0.073916\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4000: 0.373136\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5000: 0.289812\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Test accuracy: 93.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Changes\n",
    "---------\n",
    "Added dropout with rate 0.05 (5% probability to drop a neuron)\n",
    "Increased the training iterations to 15k\n",
    "Added 1 more FC layer\n",
    "Increased the neurons of the first FC layer to 120 and the second to 84\n",
    "\n",
    "Got to 95.0 accuracy for 15k epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrmxnRGY-m8X"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5 # Filter size 5x5?\n",
    "depth = 16 # Number of filters?\n",
    "num_hidden_1 = 128 # Size of the fully connected layer?\n",
    "num_hidden_2 = 84 # Size of the fully connected layer?\n",
    "stddev = 1e-1\n",
    "\n",
    "padding = 1\n",
    "stride = 1\n",
    "\n",
    "pool_filter_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # image size 28\n",
    "    out1 = output_size_conv(image_size, patch_size, padding, stride)\n",
    "    out1_pool = output_size_conv(out1, pool_filter_size, padding, pool_stride)\n",
    "    size = output_size_conv(out1_pool, patch_size, padding, stride)\n",
    "    size_pool = output_size_conv(size, pool_filter_size, padding, pool_stride)\n",
    "    \n",
    "    weights = {\n",
    "        'c1': tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = stddev)),\n",
    "        'c2': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = stddev)),\n",
    "        'h1': tf.Variable(tf.truncated_normal([size_pool * size_pool * depth, num_hidden_1], stddev = stddev)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_hidden_2], stddev = stddev)),\n",
    "        'o': tf.Variable(tf.truncated_normal([num_hidden_2, num_labels], stddev = stddev))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([depth])),\n",
    "        'bc2': tf.Variable(tf.zeros([depth])),\n",
    "        'bh1': tf.Variable(tf.zeros([num_hidden_1])),\n",
    "        'bh2': tf.Variable(tf.zeros([num_hidden_2])),\n",
    "        'bo': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        \n",
    "        # Conv layer 1\n",
    "        conv1 = tf.nn.conv2d(data, weights[\"c1\"], [1, stride, stride, 1], padding = \"SAME\")\n",
    "        hidden = tf.nn.relu(conv1 + biases[\"bc1\"])\n",
    "        hidden = tf.nn.max_pool(hidden, [1, pool_filter_size, pool_filter_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], padding = \"SAME\")\n",
    "        \n",
    "        # Conv layer 2\n",
    "        conv2 = tf.nn.conv2d(hidden, weights[\"c2\"], [1, 1, 1, 1], padding = \"SAME\")\n",
    "        hidden2 = tf.nn.relu(conv2 + biases[\"bc2\"])\n",
    "        hidden2 = tf.nn.max_pool(hidden2, [1, pool_filter_size, pool_filter_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], padding = \"SAME\")\n",
    "        \n",
    "        # Fully connected layer 3\n",
    "        # Flat the input data\n",
    "        reshapedh = tf.reshape(hidden2, (-1, size_pool * size_pool * depth))\n",
    "        fullc3 = tf.matmul(reshapedh, weights[\"h1\"]) + biases[\"bh1\"]\n",
    "        hidden3 = tf.nn.relu(fullc3)\n",
    "        hidden3 = tf.nn.dropout(hidden3, rate=0.05)\n",
    "        \n",
    "        fullc4 = tf.matmul(hidden3, weights[\"h2\"]) + biases[\"bh2\"]\n",
    "        hidden4 = tf.nn.relu(fullc4)\n",
    "        hidden4 = tf.nn.dropout(hidden4, rate=0.05)\n",
    "        \n",
    "        # Last layer\n",
    "        logits = tf.matmul(hidden4, weights[\"o\"]) +  biases[\"bo\"]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def compute_loss(labels, logits):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = compute_loss(tf_train_labels, logits)\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.278615\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 1000: 0.753899\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.090648\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 0.071513\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4000: 0.425282\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5000: 0.498017\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6000: 0.570223\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.133697\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.435616\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 9000: 0.126167\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 10000: 0.136328\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 11000: 0.143581\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12000: 0.213013\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13000: 0.366457\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 14000: 0.688557\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 15000: 0.326038\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
