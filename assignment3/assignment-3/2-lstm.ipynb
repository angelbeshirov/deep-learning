{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3.2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == -1:\n",
    "        return ''\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "1\n",
      "[' a']\n",
      "1\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)] # the max index in 1-hot encoding -> character\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    print(len(s))\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "s = train_batches.next()\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size]) # 1x27\n",
    "    return b/np.sum(b, 1)[:,None] # 1x27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-1e4a97d86b9f>:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294889 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "ovararrav    txewx mnbr alis l  bpm yltbzi etoqlil vdhqitthirtxkeibjzktsos hiwpe\n",
      "drumd rvtziyeghiwi nehrt fqpi  gefft ampwuoboezs  yppqcvr rdctcaxxi y tgrbldptgs\n",
      "quo  sracnaoepsnftoet tqlygizrrliafyc tun laieyaoounjnqzlesu bzzeoitlzjeh nmcidr\n",
      "f t  ekmalitxsfc i jcrmgpvce cataresgwsc lry eaorg kelvmn e brityoapaykxcuvblouf\n",
      "csqdoyarrcanften v iot rc htlgnmlhih es scts ez nueseee piszaaan nrf  lh wnc vtc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.593430 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.91\n",
      "Validation set perplexity: 10.24\n",
      "Average loss at step 200: 2.253966 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 300: 2.100439 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 400: 1.995835 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 500: 1.932106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 600: 1.907407 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 700: 1.855248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 800: 1.814946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.828467 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.824696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "je of the boliate infinity teven eight stitter iffobetion stense mus theoridel o\n",
      "h cospers wroltazisthy of inity one seven four six enorne one nine eight rangu d\n",
      "al rewilling ffove colverications tatle himan to hove and bsted that hauks argan\n",
      "hy gunsorde coulth rie over of his mehinced desixicle redrement of mecould was d\n",
      "hestoy partu ling smation useving entern aibst been new younch or bening derswif\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100: 1.772350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.749024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.731970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.747878 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.737244 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1600: 1.749038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.712913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.679946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.650666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.700323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "bicarling orage by be plimbing world the encident the famber parding destremine \n",
      "ne in the rist schies of two zero zero zero one two courth in the inerca one ind\n",
      "zers rebrewed a that the anst india and with x minade unitian the one eight yarc\n",
      "y lost signes carrablaminal linoagemenks to sender is integenity spulative optio\n",
      "ne at hose six not gporzer je pade proonedes of claver nown lidelling and kseeng\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2100: 1.690357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.678919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2300: 1.641353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.660966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2500: 1.682734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2600: 1.654410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2700: 1.658165 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2800: 1.657058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2900: 1.649660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000: 1.652846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "z pristing propers alitmencally and ilith conditinable exterping will allog patc\n",
      "oric when a puppoer as bodds in comer raber hwas tramery hathic souxhid unite ma\n",
      "manyisted of the obcalitted of translations autons culting by advantall raishond\n",
      "ch at and callaking is amlicerts derimed which heaved in vedied as and slandstal\n",
      "res latelisent with codns earlizage miked scoke two and one deffects at the wodl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3100: 1.627345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3200: 1.645392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3300: 1.638350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.670308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3500: 1.658174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3600: 1.670656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3700: 1.645914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.644862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.637587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4000: 1.652054 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "ates was oritton has one nine two eight tho nightere fonr which it aboncourally \n",
      " hohe libers sentric in crassicome the civil bevomed represents actinibut and th\n",
      "ire is celds k cals which sobaliem de projectant of conquer incuedy countrict hi\n",
      "wars farcashens mahath jultried aurure and used followds and the hoclies hehtwon\n",
      "are amookes and religion with the lair to beby grerent and terpe pespit and airr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.633484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4200: 1.633462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4300: 1.617062 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.609390 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.619640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.616109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700: 1.623173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4800: 1.627443 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.630576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.604835 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "zating the somethic e l stand mess and for gradized world overs tenest gravi by \n",
      "polity vila retempands to the reference obderenge to batrofigan welk or suncilie\n",
      "vay the voster intashed gruch state s empire from the on there led s subjectling\n",
      "fecttin instated on the veararge betweth number not still diridans contains in o\n",
      "tho howeven likein after as itperation by terminion capsterbn releging the a coa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.606163 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.586186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5300: 1.573621 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5400: 1.576419 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5500: 1.564294 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5600: 1.579025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5700: 1.569249 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5800: 1.579914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5900: 1.570387 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6000: 1.543287 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "zon two one nine five one eight bouth both gno six three three eight the went th\n",
      "nespity to the vowecoss muthook one is all colthsand art work set he good presay\n",
      "x the lived in octal manabla jocesterst for stuphorishmentary contsmal nine lape\n",
      "s itss exetia of depairte run momation explement fremders undertturq prevaryde i\n",
      "ha resignoffre lict of where some boua wac doinment stas sminf widous sestainate\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.561860 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6200: 1.529001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.544281 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.538607 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.558300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.595124 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.577523 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6800: 1.599513 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900: 1.581696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7000: 1.569343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "y theve layizor ony eight probussived uses the was expents for newsiensing the l\n",
      "zero succeedlyps d deece have one eight eight one three when east steng councale\n",
      "x cude has sazer part the traoler ackisosshooking coical mode but one one start \n",
      "king player the following jolenania the cornitical shurn oversian conorqure of t\n",
      "y the granclet unciputelod field of the odjerbracing a wordk and the popolic act\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:]) # 640x27\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x27 * 27x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0])\n",
    "        forget_gate = tf.sigmoid(gate_split[1])\n",
    "        update = gate_split[2]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(gate_split[3])\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # output is 64x64\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.314170 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.50\n",
      "================================================================================\n",
      "s  e    e   w   e          e                            g                       \n",
      "f  e                           e        ee          r    e  e        e          \n",
      "s                       r                     t e     er             s       e  \n",
      "v t e  e         t  e                       e               e                   \n",
      "x    t e              t              ee   r                                e    \n",
      "================================================================================\n",
      "Validation set perplexity: 250.26\n",
      "Average loss at step 100: 2.705637 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.90\n",
      "Average loss at step 200: 2.322724 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.12\n",
      "Validation set perplexity: 9.59\n",
      "Average loss at step 300: 2.185354 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 400: 2.119677 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 500: 2.059093 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 600: 1.978895 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 700: 1.953513 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 800: 1.948961 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 900: 1.929830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1000: 1.927283 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "================================================================================\n",
      "opory s the serpons the conused fron twandh issunnst pyscicat to t letk val stys\n",
      "y one six one a one is belic wictilatie fy teen agtroratials afners garses preve\n",
      "ly to partiyov overn five mai smplica d veno in ceasia pregucive trivints in the\n",
      "k as anatiya shewwol caputity hkor bul enss alsi gso haw transing y hqe pranovic\n",
      "wer in a one zero i a beret the zerowen porg morandade sig forder nitm debenf mo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 1100: 1.881206 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 1200: 1.856229 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 1300: 1.847226 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 1400: 1.846904 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 1500: 1.833077 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1600: 1.810187 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1700: 1.794932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1800: 1.768710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1900: 1.776174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 2000: 1.752767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "ter lox repticlegher with kink perce semictor reserver of the us plact of five m\n",
      "jory is deans english in excaules defections crixitand histent by hond aigst of \n",
      "jels niterary of the was mangueth hulftater stach german earcables his for reter\n",
      "di the regond sperection one nine seven zero three hus creace two zero zero zero\n",
      "fical conisers by to teng turk len as fove anshembessions of the seven two nine \n",
      "================================================================================\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 2100: 1.761779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 2200: 1.782080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 2300: 1.779863 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 2400: 1.757044 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 2500: 1.757910 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 2600: 1.734901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 2700: 1.752624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 2800: 1.742690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 2900: 1.731734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 3000: 1.739797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "chem in these segill of herart bove new musst its effect bechave of the oftem to\n",
      "k thiod two zero of dlam has groud seg whre network this prooting and of rencorg\n",
      "kly ty the liter and terns and of the turstraling of a in the end basonine coute\n",
      "jonomation stus to of dethow into the usely it dovers longy of up briquen s zero\n",
      "e the wam aspastoo other euro fective recovsples vas of comoders american snew j\n",
      "================================================================================\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 3100: 1.706932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 3200: 1.693704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 3300: 1.704933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 3400: 1.690652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 3500: 1.731627 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 3600: 1.709523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3700: 1.707541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3800: 1.706656 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3900: 1.704595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4000: 1.698841 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "sse pleatence communy bionnationsuns foundagixs runert depilty writter sive can \n",
      "e suofessy vieted the blid to then the fects and fouw r regorns enested been leb\n",
      "us compates of juspificiational attember jechan forbercaligo meanspege handerpen\n",
      "led tand fegase detzong includs sad a stake base zero six count ameno hare scunc\n",
      "rced whet to active guerd what the is mounce with w if foth it for the instrive \n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4100: 1.671095 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4200: 1.661808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4300: 1.670442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4400: 1.654872 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4500: 1.689370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 4600: 1.673864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4700: 1.670473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4800: 1.656100 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4900: 1.662757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5000: 1.656075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "cide specited has eustern one nine six a desimsus seal thearce me with canide sa\n",
      "hag mostlu adracran arkang one dephated lite which four tominaria at british rel\n",
      "less area demip rajestate notamen at jew of hilked the limet and ordidenral has \n",
      "parts and officate th the achiclybelity or graman bensaing american one six eigh\n",
      "ner of oblestome allory a blisive uncon one two than rac living generance imager\n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 5100: 1.635481 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5200: 1.639932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5300: 1.638167 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5400: 1.634567 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5500: 1.631149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5600: 1.609155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5700: 1.623329 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5800: 1.640941 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5900: 1.629605 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 6000: 1.629015 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "ing becated ramye innamed leedance of the ghi meticishing persone conductives a \n",
      "king it city de distorys abotts to boonce possetting sin is the satility disis a\n",
      "hales and fance a and the lave was becauses cansever lonly highefions of its are\n",
      "x witherte his los vodiest souse in a referency of was nb calas underformers and\n",
      "bies from the ailly sounn an everce one five joh the dajed proctiless peryon hod\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 6100: 1.617673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6200: 1.631376 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6300: 1.632560 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6400: 1.614403 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6500: 1.601272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6600: 1.647592 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 6700: 1.610442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 6800: 1.623660 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 6900: 1.614040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 7000: 1.633809 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "ologic and highthologanys in propetes ging detark the distrauting viewards excer\n",
      "xilan ma have a arthoptures incraudion consticaf out referrell human beleve best\n",
      "variple airearic and us almangly production commer to shalan an requer nation se\n",
      "fached hudctisivans mille redies similar in the wars decramed diyezil passy no d\n",
      "x eritires four seven five winkh destingne clase to the mork new gand the projec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-4ddf6568089f>:57: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# a) - introduced embedding lookup on the inputs + feeding the embeddings to the LSTM cell\n",
    "# The simplified matrix multiplication from the above task is also used here.\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs: # 64x27 -> 64x128 (almost nothing changes the output size remains the same)\n",
    "        \n",
    "        # argmax returns 64, containing the indecies of the characters 0 - 26\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1)) \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307521 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "================================================================================\n",
      "cassp  amng sx nt  soae  en       r    r  n      zspde n   n  ai  i ekr c  e  ti\n",
      "k            ef   eisw  l    se  dio    w ea nro i     ep  e  ti  a erg  ue e   \n",
      " a ean em   do    ah  arc  asm oe id    efcwm ee        c tok    seu en       f \n",
      "aic    oee    t  ie    t et     dgtit  e  akok  t  aen   sc     y     e erem  h \n",
      "ep tq  r sgnv    str     etr e  or     ae  r oen      h ie e im r a    n      ay\n",
      "================================================================================\n",
      "Validation set perplexity: 28.09\n",
      "Average loss at step 100: 2.379922 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.55\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 200: 2.084412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 300: 1.979916 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 400: 1.924629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 500: 1.943505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 600: 1.872477 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 700: 1.850623 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.832440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 900: 1.825120 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.766943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "gers is depiticle decomplanges publis or for were molence and in unends topulati\n",
      "tically ampourtational rew fommually althor iteming on highan covers fulments vi\n",
      "jame beigt one nine duccell releney meriming politegaing hand carys anties elect\n",
      "worcks putroble detlish citc mericals to it secon hest he i plakemer afcail as b\n",
      "quate the extiens cartiming duar of axfuis there one nine eight bakmeral part on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.740335 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1200: 1.767833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1300: 1.746621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.724392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1500: 1.716882 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1600: 1.711603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.732470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1800: 1.700142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1900: 1.702917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.714290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "ning partual dikenques see mbrate the more of been stemaded by the asord coun th\n",
      "bradic usixed dyson e arsaraphlobitets regadersists a depabeda performactivlle n\n",
      "us how llandish omerists eacter arrish launtariatia cleated born enace and garks\n",
      "wivitules hactively histern either clude boins at of legatmentiam userviate are \n",
      "xistazy libeans flatms of hahother countent the using statewite ecolly in the su\n",
      "================================================================================\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2100: 1.700878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2200: 1.674330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2300: 1.684874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2400: 1.689238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2500: 1.710360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.681662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2700: 1.697529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2800: 1.657984 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2900: 1.664548 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3000: 1.670723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "bitistisle of priuroed languary drucrel used or plysig the gs pried of this isli\n",
      "ker aultomich in the unulousposed former the loplace of ur like isgardes of p lu\n",
      "boder which is to kilopserage lavedic of voried feing one grouptized with morera\n",
      "many of the unitisal reconts as grewle days recembers prohard willed i auth with\n",
      "urable here whereth pitico the notlation in apport if a with muston kshirong the\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3100: 1.671969 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.664470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3300: 1.647284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3400: 1.648321 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3500: 1.642487 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3600: 1.647333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3700: 1.646502 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3800: 1.638179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3900: 1.636898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 4000: 1.635734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "lose to its and his these rated the provide is pockailably cau in the keensivela\n",
      "adook rebadbitg is a squature wassentical wollocided the with rung revised that \n",
      "ly hughther frentates for hand initically hos adiels increars mike for the proul\n",
      " thems v palacy tend or for the spane indeguree ye seember clong is ceckllysy im\n",
      "keally islammationals produced forthel to olds ground offences nations islagers \n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4100: 1.639954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 4200: 1.630372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4300: 1.609742 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4400: 1.641891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4500: 1.650034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4600: 1.650389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4700: 1.622083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4800: 1.607349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4900: 1.623801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5000: 1.643666 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "de with a produles truss shor very viso then largery and the size considere tran\n",
      "ly hubbisticle of the smaller thampor by gree one primed to commissian that trec\n",
      "ms dichanged licearpan c work for the michle cas chros loging three zero six for\n",
      "zbun off fuet indar drascus were weight pites the endo molector plaping the mode\n",
      "onsledine one nine from ser casticame incluai lnew clossing the gausponey b one \n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5100: 1.635932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.621950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5300: 1.585792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5400: 1.583715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.570089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5600: 1.596562 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5700: 1.558379 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.566812 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5900: 1.579806 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6000: 1.549807 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "ed with oup been the of sechne winging gas because jranofts the sports of scurre\n",
      "gammeq and it open auptro clast in affacitic lasess anring one other long functi\n",
      "qual card far raviar instriative wide is music purportistom peterna matches hown\n",
      "ly med one nine five plans the coguncing system its figun team the even becague \n",
      "istruact and that nire of practicinge cultorings alcommunities him buile and a m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6100: 1.572937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6200: 1.590879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.599270 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6400: 1.630036 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6500: 1.625038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6600: 1.595062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6700: 1.580250 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.565178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6900: 1.552933 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7000: 1.568880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "ker a also four simmoring to sobrations and feell d one b frades the turnebul ge\n",
      "chor that menturbie the opperier actude metration international such experientit\n",
      "zys to got brutions and abmic of tictive domes beating town poets among for bued\n",
      "quesive of chosegmonal of service to fictions it wohough an from theant sakner v\n",
      "tific game deig fl fourth contrations to their nalobarphi endorment have known v\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) - Added a bigram-based LSTM, modeled on the character LSTM above. The model uses the same batches, except\n",
    "# for the validation batch, where at least 3 characters are needed to get output\n",
    "# The bigrams are made like this:\n",
    "# if we have input xyzt this is changed to (xy), (yz), (zt). Therefore if the input is of size n it become\n",
    "# of size n - 1\n",
    "# The labels are shifted by 2 instead of 1 \n",
    "\n",
    "# The embedding lookup is done as follows: \n",
    "# We get 2 indecies for the first and second characted in the bigram, then the index of the first \n",
    "# character is multiplied by the size of the vocabulary and the second index is added to get a unique\n",
    "# index for every pair of characters.\n",
    "\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "    # batch size is 64, train data contains (num_unrollings + 1)x64x27\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by TWO time steps.\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs_zipped = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs_zipped: # 64x27 -> 64x128 (almost nothing changes the output size remain)\n",
    "        \n",
    "        # Convert 0..26, 0..26 into 0...729\n",
    "        # The way to do it usign the formula idx1*27 + idx2; idx1 is from 0 to 26; idx2 is from 0 to 26\n",
    "        # This way for every bigram xy the index will be different if the bigrams are different\n",
    "        \n",
    "        idx1 = tf.argmax(i[0], dimension=1)\n",
    "        idx2 = tf.argmax(i[1], dimension=1)\n",
    "        \n",
    "        # dim is 64,\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, idx1*vocabulary_size + idx2)\n",
    "        \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    \n",
    "    sample_input.extend([tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                         tf.placeholder(tf.float32, shape=[1, vocabulary_size])])\n",
    "    \n",
    "    sample_input_idx1 = tf.argmax(sample_input[0], dimension=1)\n",
    "    sample_input_idx2 = tf.argmax(sample_input[1], dimension=1)\n",
    "    \n",
    "    # Only 1 bigram -> 1 output\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, \n",
    "                                                    sample_input_idx1 * vocabulary_size + sample_input_idx2)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.287021 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.76\n",
      "================================================================================\n",
      "jy  ern oe laj rn d      ae e   i s oni     t   go o e i f r             ee   e l\n",
      "vtw    eo          e aht e             e         i k                             \n",
      "hlot    i ok e       e     e a w        e      a a i n         e sta      e      \n",
      "erteen ue   i z                     r           k   si          htrgir   e i a w \n",
      "pq         i e     s          ce ne                 m                     r      \n",
      "================================================================================\n",
      "Validation set perplexity: 30.43\n",
      "Average loss at step 100: 2.337829 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 200: 2.006639 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 300: 1.921440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 1.859646 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.789885 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 600: 1.784331 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 700: 1.770472 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 800: 1.747439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 900: 1.742080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1000: 1.708760 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ry three can enfloman exfrot a their five the touns the speces dreas factorla spe\n",
      "hcyda part england three and makes of the muchleter botted stated informican ofte\n",
      "bs bynings by the maboy bond anophhil decric mafl deffich often s gaskeudary inpi\n",
      "ohlaristicle unites is estance freven her of the amoxion often was kes in ed dnet\n",
      "ay orgoand of govirent out sur only chatle phen seose to popaid decish systems of\n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1100: 1.715253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1200: 1.707999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1300: 1.708241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 1400: 1.678510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1500: 1.668792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1600: 1.655465 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1700: 1.666596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1800: 1.683414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1900: 1.664976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2000: 1.676489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "ue arnal secompwled over to stetion first tool the gazae late men this and party \n",
      "vmer settles in ner one zero zero zero v on lawards of the these gatzhard a it in\n",
      "hca that dinti th comparinks by that in cenunder candassing the anscialile on the\n",
      "nmer the resillional volence war that and levelativelor that its other maliam mar\n",
      "qtness esturnal linengly commer to connoth root country anch in nimballa one seve\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 2100: 1.657164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2200: 1.678386 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2300: 1.656650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 2400: 1.653851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 2500: 1.662036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2600: 1.648718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 2700: 1.632613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2800: 1.629649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2900: 1.626718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3000: 1.648511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "hzilos their or perceignn pea protery in femace all perbid anomentified and efter\n",
      "vpms lil general nobed as servists by the broil see for emperor free are that say\n",
      "sred roase hell exibilad compopcs the he parguaship nage are soviet rebight ory i\n",
      "bc cars how polying force by due fills homan refire deof lia first a persi of man\n",
      "xifilm in free and see of one islar chixosgemility argenr briscykken the cated ma\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3100: 1.614152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3200: 1.632033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3300: 1.632367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3400: 1.629523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3500: 1.616164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3600: 1.635968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3700: 1.607462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3800: 1.605385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3900: 1.592910 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4000: 1.610122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "wto b the bary and and had elpc raimation i the matt burdeare to marshemercispini\n",
      "jew observed the their scholum founghapyris by tage and specisii x and garved to \n",
      "gs percet of the and abert fliphaloonia minosed thetkrage to sets west stude was \n",
      " jajenting debosh kulgiouse at programs it loes jean as algebrase idaman agreetly\n",
      "ected years facterased snia and andamging were was that of abskation pao in basso\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4100: 1.621803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4200: 1.606163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 4300: 1.575232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4400: 1.600949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.15\n",
      "Average loss at step 4500: 1.585834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4600: 1.594951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4700: 1.603879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4800: 1.599634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4900: 1.621592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 5000: 1.629236 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "ojence of konization and essider in dogisach is insivisia base the great but allo\n",
      "khalst leon is of lasted abstackentius nic may world it them played other or unde\n",
      "cn eur s histon tens it groups warno is would directon by and have lights in the \n",
      "gno position cominary the brokend party in britz voile as momes prescs and loccus\n",
      "imitely thus promis name was of the to reljgogosynery that its july for lilume th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 5100: 1.586226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5200: 1.602591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5300: 1.577602 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5400: 1.566581 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 5500: 1.565381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5600: 1.554893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 5700: 1.583161 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 5800: 1.574815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 5900: 1.583364 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6000: 1.540902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "oacher languages modinal b off drintegraphy pourt aston joything requation contro\n",
      "cies husa in zero six one one basic wast whiles many to one nine chain for common\n",
      "gxengal americanist of sques mound slown of be has popularly colociate carnal nat\n",
      "pkhaving may nator to a the two threess there poles vcppinesia a lees or most mov\n",
      "rter four a confliction sations flub international linfal an such criticist sconc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6100: 1.593276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6200: 1.587081 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6300: 1.573725 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6400: 1.592044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6500: 1.588928 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6600: 1.577875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6700: 1.569442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6800: 1.580930 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6900: 1.614512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 7000: 1.591219 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "zpre statuis her near stations of the gillian are influence of they unid cluk had\n",
      "kqthis also performs later not he was lawso bewindenome purwised to jacasum succe\n",
      "kachaniature was a northehrep of the raind has export was is h sir of the six col\n",
      "jptain assize etini the resial and ench b world although mathemica the keam of th\n",
      "xfaged to is three purli regios successivi to the player there of the plarticular\n",
      "================================================================================\n",
      "Validation set perplexity: 6.67\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2) # Need at least 3 letters for bigram model\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2) # Used as buffer\n",
    "                    # Add 2 sample letters to the buffer for input for the bigram model\n",
    "                    feed.extend([random_distribution(), random_distribution()])\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    \n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                             sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[-1])[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                      sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Added dropout to the non-recurrent part of the network (15% chance to drop a neuron)\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    drop_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "    # batch size is 64, train data contains (num_unrollings + 1)x64x27\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by TWO time steps.\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs_zipped = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs_zipped: # 64x27 -> 64x128 (almost nothing changes the output size remain)\n",
    "        \n",
    "        # Convert 0..26, 0..26 into 0...729\n",
    "        # The way to do it usign the formula idx1*27 + idx2; idx1 is from 0 to 26; idx2 is from 0 to 26\n",
    "        # This way for every bigram xy the index will be different if the bigrams are different\n",
    "        \n",
    "        idx1 = tf.argmax(i[0], dimension=1)\n",
    "        idx2 = tf.argmax(i[1], dimension=1)\n",
    "        \n",
    "        # dim is 64,\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, idx1*vocabulary_size + idx2)\n",
    "        \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        dropped = tf.nn.dropout(logits, rate=drop_rate)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=dropped))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    \n",
    "    sample_input.extend([tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                         tf.placeholder(tf.float32, shape=[1, vocabulary_size])])\n",
    "    \n",
    "    sample_input_idx1 = tf.argmax(sample_input[0], dimension=1)\n",
    "    sample_input_idx2 = tf.argmax(sample_input[1], dimension=1)\n",
    "    \n",
    "    # Only 1 bigram -> 1 output\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, \n",
    "                                                    sample_input_idx1 * vocabulary_size + sample_input_idx2)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296660 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.65\n",
      "================================================================================\n",
      "yf   qe  i  b  i   ea f  jv    e z    l dd    c b    z  a g d  d  hm e p  a  e   \n",
      "hbfa  z  j  t   p  is   x t  le  wni   r r c  r e t    ee  g  i i d   o l  m     \n",
      "fk  u  e  i g e we  c   l r he   a  e  g wht  c   e b ee  a   e  i w me  e  y  y \n",
      "mo s e    r  g wp po s  t  c   m   r lz  b    we t  p  t  d   d  t e e  m  t e  s\n",
      "fnb  e   r cm a  e   y hn esoes  dd  t  k h     e  y e   we  a  t  g n  e  i  q  \n",
      "================================================================================\n",
      "Validation set perplexity: 56.66\n",
      "Average loss at step 100: 2.480084 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.86\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 200: 2.147658 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.16\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 2.070184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 400: 2.006303 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 500: 1.983184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 600: 1.942267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 700: 1.929340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 800: 1.892194 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 900: 1.883402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 1000: 1.876062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "five situde one nine trlock player one this that movoach onclanded calleken onain\n",
      "dul the bropinever pii succentame in the prince obbereat proziod cruvluzt were si\n",
      "king specisln of chanif useal conce goohil techrents in side kut of kadei fildded\n",
      "lxed the tal three one nine in zero zero zero nine onine seven numy sokeet pols s\n",
      "nd onwaded than deaciiv  somee one using the four knuarate year sent of full grol\n",
      "================================================================================\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1100: 1.864285 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 1200: 1.851494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 1300: 1.835795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1400: 1.836489 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 1500: 1.849312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 1600: 1.845112 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1700: 1.817713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1800: 1.844799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1900: 1.841841 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2000: 1.812597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "ukoopk others arbetch confrunirel culture federao rooming and wistant charrimage \n",
      "o also one johneechn one obmrain typickhed down on computers two zero polings rep\n",
      "y wing moble divis a wen choes alth you agary ip astertanic texaften cuverthase h\n",
      "ffice computermins orian succps a particoeigh allster of exclusion was w care awa\n",
      "pt lendate sed progracouu slisters ascrels bxwynbiguelatin olding in well was the\n",
      "================================================================================\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2100: 1.805676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2200: 1.788019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2300: 1.826236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 2400: 1.812130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2500: 1.786635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2600: 1.777209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2700: 1.771382 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2800: 1.777352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 2900: 1.761252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3000: 1.766344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "hbol in the unflused of quartoles come to be empire one fatued eight the used efj\n",
      "yandark of cohnication egypgcreakey two wato as at this use is piers chines the t\n",
      "hy calacii wth john arre milase ipeoper the tokhax clears chyry diffice included \n",
      "xutancie glow word sovietyre oes of the laq weir quwcsiome rubli m school gamenre\n",
      "aftworld molity thophe sovers been tabm resences church centy four acrinis folonl\n",
      "================================================================================\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 3100: 1.793791 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 3200: 1.782852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 3300: 1.771187 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3400: 1.773696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 3500: 1.760135 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3600: 1.737541 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 3700: 1.751790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 3800: 1.758003 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3900: 1.772351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 4000: 1.754360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "jya review slanize anguune chine noter and emerties and stars by permisf hics hun\n",
      "und one work on selesside sormation indid study duts chillanguager with entirscie\n",
      "rve ears in time time the list pro the fik americm and pi of terest sperfor made \n",
      "uji avf sac common increana solages that champim toclasnesis is the criting an mm\n",
      "fs rica eucigned would its of com some storan cality it for jack are the cominanu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4100: 1.768177 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4200: 1.743654 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4300: 1.746938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4400: 1.752200 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4500: 1.755394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 4600: 1.744840 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4700: 1.747263 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 4800: 1.759026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 4900: 1.744407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5000: 1.759398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "fliring as car calist and teaheintephazod deb arriore change withht stitude to ca\n",
      "unfathe of the classe find al in traintings reterrdin cereation ences frecloser d\n",
      "ychablecutence bomepy leffected ans incressed ves prohibits xed to images bites m\n",
      "yudniv kuwas aw harry all in prove as the sarix crowders of centurnifiet over les\n",
      "ges by for imague attributed by e equarist and creota structribut start called to\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 5100: 1.738269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5200: 1.732191 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 5300: 1.730217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5400: 1.711145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5500: 1.714487 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 5600: 1.732404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5700: 1.712008 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5800: 1.705881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5900: 1.720715 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 6000: 1.717726 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "cjace and formated the is sectros exammentics letashed roufailes designafa takill\n",
      "kqeo decbded as him tricty culture discown sotees or forces becone eight four wro\n",
      "bn bank wus acceunx saig people in the rly neithery known abaoyuel ong of it mash\n",
      "oetrol themen calpc a ceeks and josepmel includee tayyings called and outernamall\n",
      "ght dwidope movies south been ervicto wellity born debfowner finalist orealf been\n",
      "================================================================================\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6100: 1.745298 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6200: 1.717208 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6300: 1.717940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6400: 1.745771 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 6500: 1.758196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 6600: 1.729378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6700: 1.731196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6800: 1.711263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 6900: 1.686615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 7000: 1.726000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "id asso vered just fared shortles tradeasn fronignet in meler upway that of espee\n",
      "kving france fared or of a and dagk object geutlantis novel and parts internative\n",
      "overved where followide action of the monapht the open only the nestist tolm invi\n",
      "xg to kium do mobandard city and pyr to shoet coyed in the deburty activations us\n",
      "gare austrative the ners is kes if increcially mulegally usupportein the its for \n",
      "================================================================================\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 7100: 1.719658 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 7200: 1.714825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 7300: 1.727365 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 7400: 1.716196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 7500: 1.714133 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 7600: 1.700112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 7700: 1.712503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 7800: 1.730278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 7900: 1.731574 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 8000: 1.722375 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "jweld us not heale given see by bate numan and comwlor simal sosto rand drinine f\n",
      "y prouhive anine burrence of explored tought essed or only serviews weekneral soc\n",
      "mloi the uuuuwtly as spita in had on corregione of engine effengeno lost to cultu\n",
      "cutbon and commism was edacysation evrestingly fockla goverf the henlate was apli\n",
      "id this to technology suppa invishnably bic sexually hanni of natures has the one\n",
      "================================================================================\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 8100: 1.702544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 8200: 1.705261 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 8300: 1.722161 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 8400: 1.718526 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 8500: 1.728610 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 8600: 1.727318 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 8700: 1.721817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 8800: 1.729608 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 8900: 1.709125 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 9000: 1.719609 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "dea dof goversian second maciply could behavak of other the viction new and been \n",
      "ska losition cworkighting two eight nine two jun add levels number us the orn tod\n",
      "uying beman babel goeon recobtates than ieved in tallogs gralls obtaikieved the a\n",
      "qst those one nine nine eight eight six two two two four zero six gated cilly bee\n",
      "ws funitately in which will egypbeliends in the death inhase walhely singer diel \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.48\n",
      "Average loss at step 9100: 1.723326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 9200: 1.746263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 9300: 1.735722 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 9400: 1.709109 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 9500: 1.728671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 9600: 1.723109 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 9700: 1.726572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 9800: 1.729513 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 9900: 1.698971 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 10000: 1.716232 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "zfrien saint hppoint cuments all ficty p warne him from and sides tcami signing s\n",
      "sqoom much palem to powered anstorkatring in hely critimated sch as world founder\n",
      "uly a gaor army to loashsea s dom deplemations in ignetic mundre budence atterich\n",
      "noutrographic  sare th and present by engin vular was the union unically one four\n",
      "xwor wants and seven one n four numbage and astates works strxeye euhth tosban eh\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 10100: 1.730742 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 10200: 1.729894 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10300: 1.720477 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10400: 1.734653 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10500: 1.747333 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10600: 1.696512 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10700: 1.709930 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 10800: 1.725783 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 10900: 1.732343 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11000: 1.700513 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "fhik vaker links dhisticer to sustice socional first chesman allion of the l mean\n",
      "ne remelidad all end essocam lused under zero micals that both are to be two zero\n",
      "equit and had of buglous into it and general gaziloyal an enconstieved they gover\n",
      "udes drarage umposelihaely its livist is four two isbn four two than in assabc so\n",
      "ctuted to breabn without most befored unlalannus to the leading lipk gosee centle\n",
      "================================================================================\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11100: 1.695773 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 11200: 1.689314 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11300: 1.688351 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11400: 1.694378 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 11500: 1.699994 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 11600: 1.668567 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 11700: 1.669817 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11800: 1.698603 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 11900: 1.682064 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 12000: 1.677744 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "xying matholick icdode pataly the death emonailism and the redapitalist on alves \n",
      "ooks encept ssynoselited that they a n blatem e meaded the physice letitan times \n",
      " each  systead two company in officient eles that the fid suggen in mecharsy gine\n",
      "port ajau two earl origan sydrozwee crissed restari puote szrlegian archives as a\n",
      "wnbonist rozonatrony if mics and was untrone prestruct if the archirth languaged \n",
      "================================================================================\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 12100: 1.669837 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 12200: 1.698827 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 12300: 1.690143 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 12400: 1.719075 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 12500: 1.700371 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 12600: 1.680877 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 12700: 1.691262 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 12800: 1.697396 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 12900: 1.722793 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 13000: 1.693935 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "qying the allmer six three nine three history mikem simolic retarieve centrafis u\n",
      " novems soon the clain begnet proproved norther s effect them industraced on phys\n",
      "tuburges a was each degrely as the halist wi mes a dispation from fact of while f\n",
      "zhousalbal devoland emr ls world by was thof no so cities toms latermed at symbol\n",
      "yqrnosity the orbician for some c involution the can hate simplin since one seven\n",
      "================================================================================\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 13100: 1.701164 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13200: 1.733329 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13300: 1.717389 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13400: 1.725664 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13500: 1.733617 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 13600: 1.724751 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 13700: 1.690145 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13800: 1.677099 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13900: 1.701089 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 14000: 1.701444 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.24\n",
      "================================================================================\n",
      "troni pocram of the brime idegal before a general served it goidge of had thir ba\n",
      "nqees and some by recountingdom cappsever to also and bificet king force where in\n",
      "pnst acpunclostures the kermento of to one miller fran haftned administin bride w\n",
      "qsteadar time for the colord the prossaw sound of each seek priction th ka party \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bqce pense to partly life resultiple down the define ensurrouxyus a mzle er needs\n",
      "================================================================================\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 14100: 1.719881 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 14200: 1.715717 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 14300: 1.706301 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 14400: 1.725723 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 14500: 1.741789 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 14600: 1.725941 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 14700: 1.734592 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 14800: 1.725109 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 14900: 1.725232 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 15000: 1.714423 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "jange eas are to not multind brough flaxed to by texed to culture indepter other \n",
      "fy distant to the reachckodher only ps etondo the pasteting fy technial tradered \n",
      "lnmen cointariation the figuyeration prodormulat have and one mssion featurt impr\n",
      "vbut the usual opment of the retailed korn for yer too ph at althritic manyslase \n",
      "pwary lcs in reptal dauling able until one nine five seven zero basis governett d\n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 15001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2) # Need at least 3 letters for bigram model\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        feed_dict[drop_rate] = 0.10\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2) # Used as buffer\n",
    "                    # Add 2 sample letters to the buffer for input for the bigram model\n",
    "                    feed.extend([random_distribution(), random_distribution()])\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    \n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                             sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[-1])[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                      sample_input[1]: b[1],\n",
    "                                                      drop_rate: 0.0})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15360000  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "batch_size = 128\n",
    "rnn_size = 50\n",
    "num_layers = 2\n",
    "embedding_size = 64\n",
    "learning_rate = 0.001\n",
    "sequence_length = 7\n",
    "\n",
    "end = batch_size*6*20000 # to get ~20k batches with the assumption that average word length should be around 6\n",
    "train_text = text[:end]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example raw data:\n",
      "['', 'anarchism', 'originated', 'as', 'a']\n",
      "Example sequence\n",
      "[[], [0, 13, 0, 17, 2, 7, 8, 18, 12], [14, 17, 8, 6, 8, 13, 0, 19, 4, 3], [0, 18], [0]]\n",
      "\n",
      "\n",
      "Example output\n",
      "[[], [12, 18, 8, 7, 2, 17, 0, 13, 0], [3, 4, 19, 0, 13, 8, 6, 8, 17, 14], [18, 0], [0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dictionaries(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<END>']\n",
    "\n",
    "    words = list(string.ascii_lowercase) + special_words\n",
    "    dictionary = {word: word_i for word_i, word in enumerate(words)}\n",
    "\n",
    "    return dictionary, dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "# build dictionary\n",
    "dictionary, reverse_dictionary = create_dictionaries(train_text)\n",
    "\n",
    "# Convert each word to dictionary representations\n",
    "x_ids = [[dictionary.get(letter, dictionary['<UNK>']) for letter in word] for word in train_text.split(' ')]\n",
    "y_ids = [x[::-1] for x in x_ids]\n",
    "\n",
    "print(\"Example raw data:\")\n",
    "print(train_text.split(' ')[:5])\n",
    "print(\"Example sequence\")\n",
    "print(x_ids[:5])\n",
    "print(\"\\n\")\n",
    "print(\"Example output\")\n",
    "print(y_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell(rnn_size):\n",
    "    cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "    \n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    # Tensors with name are needed to be accessed later when the model is retrieved\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, None], name = 'inputs')\n",
    "    labels = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "\n",
    "    input_sequence_length = tf.placeholder(tf.int32, (batch_size,), name = 'input_sequence_length')\n",
    "    label_sequence_length = tf.placeholder(tf.int32, (batch_size,), name = 'label_sequence_length')\n",
    "    \n",
    "    max_label_sequence_length = tf.reduce_max(label_sequence_length)\n",
    "    \n",
    "    return inputs, labels, lr, input_sequence_length, label_sequence_length, max_label_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(inputs, rnn_size, num_layers,\n",
    "                   input_sequence_length, vocab_size, \n",
    "                   embedding_size):\n",
    "\n",
    "    # embedd the inpiut\n",
    "    embed_input = tf.contrib.layers.embed_sequence(inputs, vocab_size, embedding_size)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(encoder_cell, embed_input, sequence_length=input_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing to remove the last char and add the GO symbol for the decoder\n",
    "def process_decoder_input(labels, dictionary, batch_size):\n",
    "    ending = tf.strided_slice(labels, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], dictionary['<GO>']), ending], 1)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dictionary, embedding_size, num_layers, rnn_size,\n",
    "                   labels_sequence_length, max_label_sequence_length, encoder_state, decoder_input):\n",
    "\n",
    "    vocab_size = len(dictionary)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                            sequence_length=labels_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                           training_helper,\n",
    "                                                           encoder_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,\n",
    "                                                                       maximum_iterations=max_label_sequence_length)[0]\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([dictionary['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                dictionary['<END>'])\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        encoder_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_label_sequence_length)[0]\n",
    "         \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, labels, lr, inputs_sequence_length, labels_sequence_length, \n",
    "                  max_label_sequence_length, vocab_size, embedding_size, rnn_size, num_layers):\n",
    "    \n",
    "    _, encoder_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  inputs_sequence_length,\n",
    "                                  vocab_size, \n",
    "                                  embedding_size)\n",
    "    \n",
    "    decoder_input = process_decoder_input(labels, dictionary, batch_size)\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(dictionary, \n",
    "                                                                       embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       labels_sequence_length,\n",
    "                                                                       max_label_sequence_length,\n",
    "                                                                       encoder_state, \n",
    "                                                                       decoder_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-ab690fd1a1d1>:2: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-22-6c979b9be56b>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-22-6c979b9be56b>:8: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, labels, lr, input_sequence_length, label_sequence_length, max_label_sequence_length = get_model_inputs()\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      labels, \n",
    "                                                                      lr, \n",
    "                                                                      input_sequence_length,\n",
    "                                                                      label_sequence_length, \n",
    "                                                                      max_label_sequence_length, \n",
    "                                                                      len(dictionary),\n",
    "                                                                      embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(label_sequence_length, max_label_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            labels,\n",
    "            masks)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch, pad_int):\n",
    "    # make sure each word has the same length\n",
    "    max_length = max([len(word) for word in batch])\n",
    "    return [word + [pad_int] * (max_length - len(word)) for word in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificiation of the batch generator above, this one iterates the text and returns padded\n",
    "# word representations based on the longest word\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, labels, inputs, batch_size, input_pad_int, label_pad_int):\n",
    "        self._labels = labels\n",
    "        self._inputs = inputs\n",
    "        self._batch_size = batch_size\n",
    "        self._input_pad_int = input_pad_int\n",
    "        self._label_pad_int = label_pad_int\n",
    "        self._cursor = 0\n",
    "  \n",
    "    def next(self):\n",
    "        # Generates a single batch\n",
    "        start_i = self._cursor * self._batch_size\n",
    "        input_batch = self._inputs[start_i:start_i + self._batch_size]\n",
    "        label_batch = self._labels[start_i:start_i + self._batch_size]\n",
    "        pad_input_batch = np.array(pad_batch(input_batch, self._input_pad_int))\n",
    "        pad_label_batch = np.array(pad_batch(label_batch, self._label_pad_int))\n",
    "        \n",
    "        pad_label_lengths = []\n",
    "        for i in pad_label_batch:\n",
    "            pad_label_lengths.append(len(i))\n",
    "        \n",
    "        pad_input_lengths = []\n",
    "        for i in pad_input_batch:\n",
    "            pad_input_lengths.append(len(i))\n",
    "            \n",
    "        self._cursor = self._cursor + 1\n",
    "        \n",
    "        return pad_label_batch, pad_input_batch, pad_label_lengths, pad_input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/20435 - Loss: 0.803  - Validation loss: 0.760\n",
      "Batch 2000/20435 - Loss: 0.301  - Validation loss: 0.314\n",
      "Batch 3000/20435 - Loss: 0.176  - Validation loss: 0.153\n",
      "Batch 4000/20435 - Loss: 0.076  - Validation loss: 0.082\n",
      "Batch 5000/20435 - Loss: 0.052  - Validation loss: 0.041\n",
      "Batch 6000/20435 - Loss: 0.016  - Validation loss: 0.034\n",
      "Batch 7000/20435 - Loss: 0.025  - Validation loss: 0.020\n",
      "Batch 8000/20435 - Loss: 0.013  - Validation loss: 0.022\n",
      "Batch 9000/20435 - Loss: 0.017  - Validation loss: 0.019\n",
      "Batch 10000/20435 - Loss: 0.008  - Validation loss: 0.004\n",
      "Batch 11000/20435 - Loss: 0.014  - Validation loss: 0.004\n",
      "Batch 12000/20435 - Loss: 0.006  - Validation loss: 0.008\n",
      "Batch 13000/20435 - Loss: 0.010  - Validation loss: 0.006\n",
      "Batch 14000/20435 - Loss: 0.007  - Validation loss: 0.003\n",
      "Batch 15000/20435 - Loss: 0.002  - Validation loss: 0.003\n",
      "Batch 16000/20435 - Loss: 0.003  - Validation loss: 0.005\n",
      "Batch 17000/20435 - Loss: 0.002  - Validation loss: 0.003\n",
      "Batch 18000/20435 - Loss: 0.002  - Validation loss: 0.003\n",
      "Batch 19000/20435 - Loss: 0.002  - Validation loss: 0.002\n",
      "Batch 20000/20435 - Loss: 0.012  - Validation loss: 0.008\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Train and validation data split\n",
    "\n",
    "train_input = x_ids[batch_size:]\n",
    "train_labels = y_ids[batch_size:]\n",
    "\n",
    "valid_input = x_ids[:batch_size]\n",
    "valid_labels = y_ids[:batch_size]\n",
    "\n",
    "batch_generator_valid = \\\n",
    "        BatchGenerator(valid_labels, valid_input, batch_size, dictionary['<PAD>'], dictionary['<PAD>'])\n",
    "batch_generator_train = \\\n",
    "        BatchGenerator(train_labels, train_input, batch_size, dictionary['<PAD>'], dictionary['<PAD>'])\n",
    "\n",
    "\n",
    "(valid_labels_batch, valid_input_batch, valid_labels_lengths, valid_input_lengths) = batch_generator_valid.next()\n",
    "\n",
    "display_step = 1000\n",
    "\n",
    "checkpoint = \"seq2seq.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for batch_i in range(0, len(train_input)//batch_size):\n",
    "        \n",
    "        (labels_batch, input_batch, labels_lengths, input_lengths) = batch_generator_train.next()\n",
    "\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: input_batch,\n",
    "             labels: labels_batch,\n",
    "             lr: learning_rate,\n",
    "             label_sequence_length: labels_lengths,\n",
    "             input_sequence_length: input_lengths})\n",
    "\n",
    "        if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "            validation_loss = sess.run(\n",
    "            [cost],\n",
    "            {input_data: valid_input_batch,\n",
    "             labels: valid_labels_batch,\n",
    "             lr: learning_rate,\n",
    "             label_sequence_length: valid_labels_lengths,\n",
    "             input_sequence_length: valid_input_lengths})\n",
    "\n",
    "            print('Batch %d/%d - Loss: %.3f  - Validation loss: %.3f' % (batch_i, \n",
    "                          len(train_input) // batch_size, \n",
    "                          loss, \n",
    "                          validation_loss[0]))\n",
    "\n",
    "    # save the model state\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seq(text):\n",
    "    # preprocess text for the model\n",
    "    return [dictionary.get(word, dictionary['<UNK>']) for word in text] + [dictionary['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./seq2seq.ckpt\n",
      "\n",
      "Original Text: the quick brown fox\n",
      "  Word Ids: [[19, 7, 4, 26, 26, 26, 26], [16, 20, 8, 2, 10, 26, 26], [1, 17, 14, 22, 13, 26, 26], [5, 14, 23, 26, 26, 26, 26]]\n",
      "Input words: ['t h e <PAD> <PAD> <PAD> <PAD>', 'q u i c k <PAD> <PAD>', 'b r o w n <PAD> <PAD>', 'f o x <PAD> <PAD> <PAD> <PAD>']\n",
      "\n",
      "Output:\n",
      "Word Ids: [4, 7, 19]\n",
      "Response Word: eht\n",
      "Word Ids: [10, 2, 8, 20, 16]\n",
      "Response Word: kciuq\n",
      "Word Ids: [13, 22, 14, 17, 1]\n",
      "Response Word: nworb\n",
      "Word Ids: [23, 14, 5]\n",
      "Response Word: xof\n",
      "\n",
      "Sentence output: eht kciuq nworb xof\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence + output\n",
    "# The sentence is split into words and fed to the seq2seq model, then each word is outputed + the final sentence\n",
    "# at the end.\n",
    "\n",
    "pad = dictionary[\"<PAD>\"] \n",
    "\n",
    "checkpoint = \"./seq2seq.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    \n",
    "    # load the model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_sequence_length = loaded_graph.get_tensor_by_name('input_sequence_length:0')\n",
    "    label_sequence_length = loaded_graph.get_tensor_by_name('label_sequence_length:0')\n",
    "    \n",
    "    input_sentence = 'the quick brown fox'\n",
    "    words = input_sentence.split(' ')\n",
    "    \n",
    "    print()\n",
    "    print('Original Text: %s' % input_sentence)\n",
    "    print('  Word Ids: %s' % ([[letter for letter in to_seq(word)] for word in words]))\n",
    "    inputWords = [\" \".join([reverse_dictionary[i] for i in to_seq(word)]) for word in words]\n",
    "    print('Input words: %s' % inputWords)\n",
    "    \n",
    "    print()\n",
    "    print(\"Output:\")\n",
    "    outputs = list()\n",
    "    for word in input_sentence.split(' '):\n",
    "        word_seq = to_seq(word)\n",
    "        answer_logits = sess.run(logits, {input_data: [word_seq]*batch_size, \n",
    "                                      label_sequence_length: [len(word_seq)]*batch_size, \n",
    "                                      input_sequence_length: [len(word_seq)]*batch_size})[0]\n",
    "\n",
    "        \n",
    "        output_word = ''.join([reverse_dictionary[i] for i in answer_logits if i != pad])\n",
    "\n",
    "        print('Word Ids: %s' % ([i for i in answer_logits if i != pad]))\n",
    "        print('Response Word: %s' % (output_word))\n",
    "        outputs.append(output_word)\n",
    "    \n",
    "    print()\n",
    "    print(\"Sentence output: %s\" % (' '.join(outputs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
