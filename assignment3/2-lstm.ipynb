{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3.2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == -1:\n",
    "        return ''\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "1\n",
      "[' a']\n",
      "1\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)] # the max index in 1-hot encoding -> character\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    print(len(s))\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "s = train_batches.next()\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size]) # 1x27\n",
    "    return b/np.sum(b, 1)[:,None] # 1x27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-1e4a97d86b9f>:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297881 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "diedqe fa dolgfbohf  oztmdqlgimn wsrl  xa oadkpfdytvfniave arexzkeaghfixmodwqqgk\n",
      "pixeduractgexwguedpiwvnndhj erhwghuhjzh nsji   hyaqvgwezteslx jldnocqshnfqe  igu\n",
      "duegeiuaacxtqukqg  v rez imoosivboelycttpdoycndnacwdiajzwnywo   cwvl kfbtriipbwp\n",
      "snjnvyntrvmzk s loa  vo iiwisnjpxorttmn janou hyr  psju   nc khunanob wuhhc on g\n",
      "xaplsa m   yw zqeob  e xpybixcrsyattxodvxeeauvepy jamc goeqodnldbhjd ibwmyh ixhi\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.587257 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.89\n",
      "Validation set perplexity: 10.38\n",
      "Average loss at step 200: 2.244204 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 300: 2.095926 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 400: 2.002931 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 500: 1.936676 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 600: 1.913550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 700: 1.866742 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.820821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 900: 1.831963 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.828100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "vern sidsoucterly oreverca wigh deverf three deb licpers ofirn agout zero yous e\n",
      "ie for other sids depleftree fizs in hnjyy wixd from howeligy irea ousse of the \n",
      "e the cersed polet but whopl perpections serval cecvels crove in nut ofceste of \n",
      "jessed of the sevelos the statents reten movely deyible to everic on igis fevelr\n",
      "kel ant se propeles veles for indicing a teruaching of suppesid to the sume sint\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1100: 1.778490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.754634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.736412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1400: 1.748067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.737756 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.750893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.713080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.672264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1900: 1.649345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2000: 1.698602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "cresting infom eight in antohingori untimity deflang lattrers headians top and g\n",
      "j to aparam and bornaus sousp thrie of the inverfout ipient alternisule distmjor\n",
      "ratcorm reluthy pata of anneen same benural the de the words other dincond and g\n",
      "ker one be formable in gove nemes paitits compo striboth is see caumbinish s two\n",
      "ninf is images altonnes prike be amiraling makel the wond linen and and amaghain\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.687297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2200: 1.679224 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2300: 1.641799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2400: 1.658682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.679227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600: 1.651965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2700: 1.658141 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2800: 1.653030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2900: 1.653387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3000: 1.651562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "vingrate s newars paniswi invids book bestakings with  one nine seven schabn aug\n",
      "werd hare used becunned postored by seccle in execge accesss with well is distan\n",
      "z compablish one five five wo meunhin colvell loration was icowd has be two the \n",
      "z sandar varced than ways it vatwors that more fabrlisher when world inflogially\n",
      "ound the tratschith gamed open beem or short dece divind both farther pinish by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3100: 1.629287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3200: 1.646813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.636745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.667979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3500: 1.657632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.667511 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.645075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.644310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3900: 1.634658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4000: 1.652425 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "ly progresses arcepter the procreited shepwer productive f ole bya in the see in\n",
      "hn force malass on eight sevenlifinga bytectic lepy the under of diaforth amerer\n",
      "ke slees of the world underned it active the justs india madroly kaded the to ri\n",
      " yoring perfice exorci componstersly marious contentively enquite ria creal noca\n",
      "kes the have sen rich irervan creader of schavical unusaic anneed heacentree mou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.630889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.633958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.613768 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.607299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.614540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.611536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.619104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.628220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.632511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5000: 1.607893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "viers above and to ned soma quing his comilals have prevented develophar casenia\n",
      "d is the sen thild have than on lines by aeneance woll in one nine three five ni\n",
      "y co missia or win bouncent suec idteio allowing as the one one six wabe krourd \n",
      "umes ed romitaring occast sincencason have to history brough inquigned two s pop\n",
      "yment go subjects for the funcent and frequent in the la sound byzed his often e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5100: 1.604897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5200: 1.594117 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.580036 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.579979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.569378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.584469 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.571823 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5800: 1.580035 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.575398 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.548406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "fing forcoving oonten markmage compuicy makested collavi aberisc of this de play\n",
      "ka day ke supply corland is specimication in the convente of alternations even p\n",
      "ys sposs is part of liberian by file cuts eoplost eard roging holegn charrie tru\n",
      "ental concerds natural suttels decorder concesed sub the developist are otherst \n",
      "ally tencications erbock live trinats wapeland quarical meque heelpp soundal the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.571902 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.540189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.550933 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.543989 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.559213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6600: 1.600700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.584328 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.605460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900: 1.582047 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.576608 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "z dirage ne the prograps in the was ghown with miny norman ap the from serussifi\n",
      "les rove that it precontucial back abcacholaries the as iticte brogent two obleg\n",
      "ked aelem betwirell i deglaument known fint of pherian and mehiep maying house s\n",
      "k arrerage second companic of licling and is is more where world burdhwame time \n",
      "ver one eight zero valoboses in record has whiek and alsoon testing in y several\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:]) # 640x27\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x27 * 27x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0])\n",
    "        forget_gate = tf.sigmoid(gate_split[1])\n",
    "        update = gate_split[2]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(gate_split[3])\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # output is 64x64\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.269450 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.30\n",
      "================================================================================\n",
      "f e           e                                   e      he              e      \n",
      "w e                                                                             \n",
      "m    n                                                 e                        \n",
      "b            j                                                                  \n",
      "u                                         e                                     \n",
      "================================================================================\n",
      "Validation set perplexity: 419.37\n",
      "Average loss at step 100: 2.751703 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.67\n",
      "Validation set perplexity: 11.23\n",
      "Average loss at step 200: 2.345425 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.07\n",
      "Validation set perplexity: 9.63\n",
      "Average loss at step 300: 2.191199 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 400: 2.123705 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 500: 2.063503 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 600: 1.982811 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 700: 1.957533 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 800: 1.950435 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 900: 1.932288 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1000: 1.930118 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "================================================================================\n",
      "f the f a one janoz onger buodies one seven in prankid manie in one zero zero on\n",
      "ubned the joreo expint jimince lade hist vis s patks the non buczs placies d one\n",
      "ce sighsat of contr que four to tarch fmoner suprets wirlase denter to vix pyolr\n",
      "tly bnicung bred seigne dirs the zerces sitp in combaash redtisinationar specrev\n",
      "ry one eight nine zero zero one one seven folower refing hist b blaint where use\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 1100: 1.885709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 1200: 1.856172 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 1300: 1.852099 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1400: 1.846713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1500: 1.833605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1600: 1.807618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1700: 1.792549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1800: 1.771303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1900: 1.771020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 2000: 1.751711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "jere effect that mecknise incues and jond of siamn in the regals of ans unders r\n",
      "usiter the prokicus courtent the popsed the beidentents it greeren plapar b  its\n",
      "wone the ledwe is of correxts botwen in seenten is and formal simputes of the ch\n",
      "k in crepurht texp shy exters in the hvoth a propating noad if sosvelays of and \n",
      "istiamelly verseded actoring by the high he witht mudd collisoling pich sprement\n",
      "================================================================================\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 2100: 1.762125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 2200: 1.782769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2300: 1.780756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2400: 1.754913 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 2500: 1.753846 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2600: 1.740186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2700: 1.749386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2800: 1.742710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2900: 1.737357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 3000: 1.741623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ed conservopual akr dina elem at missiet over whodd tofeer on primanep of lant t\n",
      "y the ubent x vaespan other totiunioder motins take of thio dauboas obutroa leak\n",
      " opens two zero zero pountays of cansland at coir newsiet of opery manhark to ow\n",
      "king shar tam lishing sain co from makado floborige reformate of c flect worres \n",
      "bales in the visiary mariel catey two two cational on musing presiverens fruncib\n",
      "================================================================================\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 3100: 1.709867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 3200: 1.691496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 3300: 1.699343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3400: 1.689882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3500: 1.729742 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3600: 1.702234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3700: 1.707536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3800: 1.704836 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3900: 1.703390 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4000: 1.692745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "quation ione the langory keressibmerage topiscors cacue for dees these centry pl\n",
      "e gri factorut slomely langary resumplan froed the porke arlabur bain for the ou\n",
      "ing the prify anophervis tfbughlarth eight prupress the freech to delws but auth\n",
      "joge centron antically sutz lupress issaic purt from the prentoe effcciem one xa\n",
      "jy internation from eultions if art ohain one thus emman tempersed one four them\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4100: 1.668901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4200: 1.662688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4300: 1.665257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4400: 1.657473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4500: 1.687997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4600: 1.669574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4700: 1.671592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4800: 1.656366 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4900: 1.665211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5000: 1.657601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      " can plocke the imhil have at mil clath messia ceracusual two seiments the shoul\n",
      "on or their accured as on gieldoun of mod in critione of agremens to humd despon\n",
      "ing cmery recresic rumung that of the munnetien under cannon fashilar terrizalit\n",
      "ken plast with polligery theler kningody of willy aboriwalds capantb only fookla\n",
      "e and in the of all that of this with holted it son same was their special the d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.629127 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5200: 1.636313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5300: 1.635824 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5400: 1.639705 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5500: 1.634184 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5600: 1.603168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5700: 1.617246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5800: 1.642198 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.621067 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6000: 1.623290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ge roso asholice was resian it colter effecttely compace word disid all is atter\n",
      "ly feature wanyabalia fallo anald might commen the declice ternet of hin smonts \n",
      "ult st albocticnse xeft one five list a serformatives so sidiate as heb jadibell\n",
      "s intharving chrichid tacy fegied god shoblis to be disecovies by uspo the lup a\n",
      "der for the arictor wed tonns craims like propiction opfented cold hnotap or dir\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.618539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6200: 1.625099 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6300: 1.622100 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6400: 1.608616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6500: 1.598722 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6600: 1.640965 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6700: 1.612418 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6800: 1.617546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6900: 1.614468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 7000: 1.627600 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "quer steric with geln s references totapitio that but it releps american the rec\n",
      "geron stare of dum m actar comman amplichabality plansa frib thougher idery eut \n",
      "y agolp ahycheasting vicomouse mpty ilco ottheration and in the ps important als\n",
      "pifliby and government and indesoriation in a nax jaups too splohthoes cultional\n",
      "s after that on unitly dares metaly lieabe as was publity and was budzctive be t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-4ddf6568089f>:57: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# a) - introduced embedding lookup on the inputs + feeding the embeddings to the LSTM cell\n",
    "# The simplified matrix multiplication from the above task is also used here.\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs: # 64x27 -> 64x128 (almost nothing changes the output size remains the same)\n",
    "        \n",
    "        # argmax returns 64, containing the indecies of the characters 0 - 26\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1)) \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.316263 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.56\n",
      "================================================================================\n",
      "h o  p r at ee h en e n a   n cn r z s tn  j d h x e e y  e l e  me  j l dn v f \n",
      "ca g v r f cd t   t    o  le  e  ene k i ns j    a    t   kce g cn r ne l   a   \n",
      "e h t n o    i z r cu  t o h e h  d v n i rea e r ol po h js pn i z    s     o j\n",
      "n e a   t  f kf j es ol  r mr af   v e     es v  o s a r xe oi m ai uhnee  h ee \n",
      "wen a e te q e i di  je u t s j p a  o   r e  a  we l  b n  et f t c n ben gw w \n",
      "================================================================================\n",
      "Validation set perplexity: 64.75\n",
      "Average loss at step 100: 2.449248 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.18\n",
      "Validation set perplexity: 9.68\n",
      "Average loss at step 200: 2.137464 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 300: 2.036882 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 400: 1.979702 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.986451 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.912188 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.893581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 800: 1.871208 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 900: 1.861256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.794173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "hexust one nine to norment out demperor throns and one nine or populue of mub th\n",
      "dued in on a cernemmerder oxoliver his ducting logogs to iha sible alk biver jof\n",
      "re fasuter lexcimoon or borme the gen thana out memusts hored seet more two nine\n",
      "bols falleapps fadiew the the they of only assertion in beix only exparthually a\n",
      "elonglize out anz divido nequapera excultor prinaliter had and ighth cable of ye\n",
      "================================================================================\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1100: 1.770828 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1200: 1.796074 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1300: 1.768378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1400: 1.744978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1500: 1.736214 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1600: 1.726431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.754437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1800: 1.714786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1900: 1.717151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2000: 1.726293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "dide and oalsettments dcass rame their colight rapitested to largaedif a lestry \n",
      "enses lumid is was wothenn porters patentaffery deqvirvelials becamesty maived g\n",
      "declayo three the ak the vast the to the teatergen sijust indell ponselstry dyes\n",
      "ve arge jawere as fortlest settery orved tos harred arelobateria thee axhatest o\n",
      "stite debertatester verted aftah ppeaie inflalue loled reserted nectrowly hying \n",
      "================================================================================\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2100: 1.709531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 2200: 1.689201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2300: 1.697572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2400: 1.695519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 2500: 1.717492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2600: 1.688194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.701089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2800: 1.664620 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2900: 1.672705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3000: 1.675884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "xogi most termers legd fferce the many ward tours french provide ks peteropable \n",
      "med isitue jabuaries that deire athouted of covard legened has warmusion efrohn \n",
      "velop is sinn as scales of japy tegetral casling outsity ilderf sourdd corpitati\n",
      "windowes dorreed englimiging gan hearts fet fing who germ hmofsoigin king mollin\n",
      "concecection for resich will then is commongulabionizity arg the prepusinative g\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3100: 1.674615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3200: 1.675384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.654614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3400: 1.657676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3500: 1.650186 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3600: 1.651112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3700: 1.654038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3800: 1.645785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.638912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4000: 1.639881 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "zero winstonally and six four r interreh with compting in two fourth armemal att\n",
      "yunscresson of the spectral are into partes the amerhican publically for bot was\n",
      "form intereleve resided severe sugper of in shiressional ports appions time in e\n",
      "kaca applched labled pursogo group a coversming applical ireptor condeming was s\n",
      "increetly remently in one eigholing that polike odaether in to deight try colnum\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4100: 1.638229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4200: 1.626381 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4300: 1.610852 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4400: 1.643280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.43\n",
      "Average loss at step 4500: 1.649103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 4600: 1.653218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4700: 1.627064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4800: 1.609953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4900: 1.624361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 5000: 1.646874 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "zered also wet carligoccy part etholiania had light variasge agedus can cre but \n",
      "he lattaner the extite thatuanic mis much simple it air the show untly voliti on\n",
      "minizbranesi many to expr theying old the similar of the the bigil under the pos\n",
      "spoundagianian the in  mater its typites after of stanuis nearia for increason g\n",
      "oring diama tv biouldian metals uses for quent to the at considerition execution\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 5100: 1.638454 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5200: 1.629533 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5300: 1.587722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5400: 1.591536 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5500: 1.583606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5600: 1.605299 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5700: 1.564529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5800: 1.571243 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5900: 1.587679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6000: 1.557909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "nament spacne anfence that and ihrthgetraly sixthed mis fluthing one six five ei\n",
      " funric the lists liques the area three of two in one eight while of by longs wh\n",
      "universal avop is other president than make of the kalthard one six ssybrids boo\n",
      "zing t permanent of includes two zero in one seven t yeadibesm channocra britom \n",
      "by because and hafe americand the liance the firlits  one nine nine zero gracker\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6100: 1.580749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.600916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6300: 1.613030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6400: 1.639519 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6500: 1.634822 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6600: 1.600242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6700: 1.589204 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6800: 1.571895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6900: 1.566035 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 7000: 1.579231 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "hesions who douck the resercined soum difficer and di one nine nine sngle elemer\n",
      "nen and con technoly discot of this been tabo espection ancientuate and liqued p\n",
      "caspoinaidliguanian on covide in genotination earth as attenglar queass in relig\n",
      "xed libinovian doing the multi rul lood end and usure as moverse herpenments ama\n",
      "quemen school and the that all hellese of gasuance the findelar hydrotnern sings\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) - Added a bigram-based LSTM, modeled on the character LSTM above. The model uses the same batches, except\n",
    "# for the validation batch, where at least 3 characters are needed to get output\n",
    "# The bigrams are made like this:\n",
    "# if we have input xyzt this is changed to (xy), (yz), (zt). Therefore if the input is of size n it become\n",
    "# of size n - 1\n",
    "# The labels are shifted by 2 instead of 1 \n",
    "\n",
    "# The embedding lookup is done as follows: \n",
    "# We get 2 indecies for the first and second characted in the bigram, then the index of the first \n",
    "# character is multiplied by the size of the vocabulary and the second index is added to get a unique\n",
    "# index for every pair of characters.\n",
    "\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        \n",
    "    # batch size is 64, train data contains (num_unrollings + 1)x64x27\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by TWO time steps.\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs_zipped = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs_zipped: # 64x27 -> 64x128 (almost nothing changes the output size remain)\n",
    "        \n",
    "        # Convert 0..26, 0..26 into 0...729\n",
    "        # The way to do it usign the formula idx1*27 + idx2; idx1 is from 0 to 26; idx2 is from 0 to 26\n",
    "        # This way for every bigram xy the index will be different if the bigrams are different\n",
    "        \n",
    "        idx1 = tf.argmax(i[0], dimension=1)\n",
    "        idx2 = tf.argmax(i[1], dimension=1)\n",
    "        \n",
    "        # dim is 64,\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, idx1*vocabulary_size + idx2)\n",
    "        \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    \n",
    "    sample_input.extend([tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                         tf.placeholder(tf.float32, shape=[1, vocabulary_size])])\n",
    "    \n",
    "    sample_input_idx1 = tf.argmax(sample_input[0], dimension=1)\n",
    "    sample_input_idx2 = tf.argmax(sample_input[1], dimension=1)\n",
    "    \n",
    "    # Only 1 bigram -> 1 output\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, \n",
    "                                                    sample_input_idx1 * vocabulary_size + sample_input_idx2)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.330346 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.95\n",
      "================================================================================\n",
      "wfs i  ey      eo    ea     nt  n  d      n     o mr    efa                  e   \n",
      "wwo   rm  b  i    o    i               etidi c        ey    f       ne           \n",
      "mpi   c e                               rl oie         s b     t                 \n",
      "yyo         n     n   e    r                               e          e      e e \n",
      "ulq xn   i        ee    e          eo in       en                            i   \n",
      "================================================================================\n",
      "Validation set perplexity: 64.99\n",
      "Average loss at step 100: 2.367137 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.19\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 200: 1.999335 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 300: 1.910705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 400: 1.852846 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 500: 1.784331 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 600: 1.786356 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 700: 1.759637 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 800: 1.744272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 900: 1.734379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1000: 1.700823 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "qxv e the not sometions infer the soognes man constructrees worldific diversited \n",
      "bc goalient of believed a mas zero belike off erion by the p of the coather chrac\n",
      "own conderial realistoryd dancoad offen entible ezenions brities exive spec alcul\n",
      "guical borno exfrom oke moted is meane of olled to popy of a binich formed from t\n",
      "st new eations to direa reds as and hyet and the by threms of footh the agric yar\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 1100: 1.708970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1200: 1.703206 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 1300: 1.702497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1400: 1.673180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1500: 1.667783 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1600: 1.653594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 1700: 1.660416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 1800: 1.678205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 1900: 1.658323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2000: 1.669997 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "ujl nine the mag set dia set leagly lashitera or gend to destan us two chattas pe\n",
      "kb legarb king to somet ccipassica a wale chinidate in to ong noved on meams budd\n",
      "ihy was as ash paten statition largarease genetter admen could untacy tripter amo\n",
      " populard nout defive agrestatical three and languaur of the first laster in ager\n",
      "ix intered on pacted by the comprian one eight zero zero zero zero six yearconthn\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 2100: 1.658314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2200: 1.674737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2300: 1.652779 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2400: 1.648713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 2500: 1.656899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 2600: 1.647542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2700: 1.630288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2800: 1.634343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2900: 1.628721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3000: 1.648589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      " of tric of the resolutions goven of xpluese or jsloged continue world somest of \n",
      "or the scentrace arountoyer of the glebrose of okazakht caoess in listicil connec\n",
      "yrue of the johnsions outps progremersub used by hohistan questy revoluctreat hou\n",
      "zuments engine see by the mostpress arishing britechnorn example it alkingling ne\n",
      "cdtle els result tho tree holers as looked in one five sevex livinres emxist wir \n",
      "================================================================================\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3100: 1.612952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3200: 1.627827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3300: 1.631483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 3400: 1.624207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3500: 1.607019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3600: 1.632635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3700: 1.597464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3800: 1.600757 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3900: 1.584461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4000: 1.605815 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "djsds highr a extresidential and to and compl bother had be competit s initish to\n",
      "duce and manology particularl planety hadmission of than has nest steakful advan \n",
      " paracticies interphirebarberttchingaria rovision farth d mainom thematich al tha\n",
      "dzation calpple uhumalt culture by god have on the jer oriest of guise and domene\n",
      "gkra than is knel for fil ad wou of ulynpcs origences as alternerved theory to nf\n",
      "================================================================================\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4100: 1.617716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4200: 1.605192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4300: 1.568485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4400: 1.592641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4500: 1.581980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4600: 1.583833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4700: 1.598506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 4800: 1.591468 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4900: 1.616814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5000: 1.622596 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "i sided accoassers by rain in five five nine nine one g ths to of cusea is to bor\n",
      "kring andraged more relatefershtcreat it intercers was a indire after millse that\n",
      "y psychand  usings and hittment recturing is borrisk of stelled one two mining ad\n",
      "fequenecames vering intellivers to m the ckency to priq brank buillss achieved th\n",
      " more into cults their fa up color but the nologistive s studies such in the bot \n",
      "================================================================================\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 5100: 1.584234 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5200: 1.594369 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5300: 1.571222 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5400: 1.558053 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5500: 1.554709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5600: 1.543785 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 5700: 1.580397 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5800: 1.567624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 5900: 1.577947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 6000: 1.538966 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "wcroboum traves ares cumentation links from tley had communists notably in one ni\n",
      "ner a willost natural government and will mark yearp she based for in the order o\n",
      "b two padons are stative and in the labbally zi essed generable one seven nine fi\n",
      "vg s becompany goods all more i three to move the europe inst seams with for the \n",
      "gqo alerop have prodoves w far in regrea aski two zero four three critical differ\n",
      "================================================================================\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6100: 1.588024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6200: 1.584528 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6300: 1.570829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6400: 1.582245 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6500: 1.584688 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6600: 1.570860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 6700: 1.558525 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6800: 1.575490 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 6900: 1.605991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 7000: 1.586719 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "jp everymenberion met weaters and josmatical rocks came without lia for mit the b\n",
      "equent s a versed ao or one year indian six six zero eight seven six six eight th\n",
      "gji ship vest for opponent with it the language numbers rafficimes econment origi\n",
      "e eliteen addition is an that carally at inature as hisensions huenia causles hav\n",
      "ek museum the men to science for the anger imove scensuest was a linell elected v\n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2) # Need at least 3 letters for bigram model\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2) # Used as buffer\n",
    "                    # Add 2 sample letters to the buffer for input for the bigram model\n",
    "                    feed.extend([random_distribution(), random_distribution()])\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    \n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                             sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[-1])[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                      sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Added dropout to the non-recurrent part of the network (15% chance to drop a neuron)\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0)) # 27x128\n",
    "    \n",
    "    num_nodes = 64\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1)) # 27x256\n",
    "    ifcob = tf.Variable(tf.truncated_normal([1, 4 * num_nodes])) # 27x256\n",
    "    \n",
    "    drop_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob # 64x128 * 128x256 -> 64x256 -> split into 4 64x64\n",
    "        gate_split = tf.split(gate, 4, 1) # split into 4 arrs across dimension 1\n",
    "        \n",
    "        input_gate = tf.sigmoid(gate_split[0]) # 64x64\n",
    "        forget_gate = tf.sigmoid(gate_split[1]) # 64x64\n",
    "        update = gate_split[2] # 64x64\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update) # 64x64\n",
    "        output_gate = tf.sigmoid(gate_split[3]) # 64x64\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state # 64x64\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "    # batch size is 64, train data contains (num_unrollings + 1)x64x27\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by TWO time steps.\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs_zipped = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs_zipped: # 64x27 -> 64x128 (almost nothing changes the output size remain)\n",
    "        \n",
    "        # Convert 0..26, 0..26 into 0...729\n",
    "        # The way to do it usign the formula idx1*27 + idx2; idx1 is from 0 to 26; idx2 is from 0 to 26\n",
    "        # This way for every bigram xy the index will be different if the bigrams are different\n",
    "        \n",
    "        idx1 = tf.argmax(i[0], dimension=1)\n",
    "        idx2 = tf.argmax(i[1], dimension=1)\n",
    "        \n",
    "        # dim is 64,\n",
    "        i_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, idx1*vocabulary_size + idx2)\n",
    "        \n",
    "        output, state = lstm_cell(i_embedding, output, state) # Passing the embedding 64x128\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "      # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # 640x64 * 64x27 -> 640x27\n",
    "        dropped = tf.nn.dropout(logits, rate=drop_rate)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=dropped))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    \n",
    "    sample_input.extend([tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                         tf.placeholder(tf.float32, shape=[1, vocabulary_size])])\n",
    "    \n",
    "    sample_input_idx1 = tf.argmax(sample_input[0], dimension=1)\n",
    "    sample_input_idx2 = tf.argmax(sample_input[1], dimension=1)\n",
    "    \n",
    "    # Only 1 bigram -> 1 output\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, \n",
    "                                                    sample_input_idx1 * vocabulary_size + sample_input_idx2)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.355935 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.83\n",
      "================================================================================\n",
      "ek e      n e r p   e  y  e  o   esca      l m rt  sla q                at  nesak\n",
      "yjdvh bsa    i  i  dt z     e     i  r ee  snbb o     i     i  ae  ea ie  az     \n",
      "gf  lwi  w e       il idv   s   eohge  c       o      en k         x o     e   ad\n",
      "ffe ntr   l o   au w  piv e   c    n     sa e   a a ei        nn n a     o  a uin\n",
      "ryi  sk de u nr   e  e        x sen dn lml a       o      i  x be    at  ssdha in\n",
      "================================================================================\n",
      "Validation set perplexity: 35.65\n",
      "Average loss at step 100: 2.479287 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.60\n",
      "Validation set perplexity: 9.71\n",
      "Average loss at step 200: 2.140259 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 300: 2.053326 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 400: 1.994760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 500: 1.978417 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 600: 1.931572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 700: 1.922287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 800: 1.880663 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 900: 1.879745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1000: 1.868404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "ey two bits of anxrzpegaeen begicks one for three one nine the perna of the memrr\n",
      "atockhdk two seven one eignine unions a purience one eight reginaley of the fids \n",
      "from and monotion for he court movermers becon the conceils leashrt the jater of \n",
      "mes neol was to generatium socoing whomonically in the nove of tile and cle eight\n",
      "c ignose onctlond eva suland  not buation longbcsbw eethers seven ownmath schrist\n",
      "================================================================================\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1100: 1.854952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1200: 1.846425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 1300: 1.830373 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 1400: 1.832676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1500: 1.853097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1600: 1.842539 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1700: 1.820808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1800: 1.846042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1900: 1.842578 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 2000: 1.805745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "zdency no uxsne qutc up simpluuys are prisficium platiclly of selpilm placne were\n",
      "dpea democialry wtch wyvut ii same and cluni are issefue eight five zero line eig\n",
      "yfes requost ic adaxds lthina quarker cart birolong priclen lea in obserlard neig\n",
      "wpria from on its jewal bacad a fadmklh ho faccaoinge ofs applirity crance oerhaw\n",
      "km new offic cenger herporitis universia rbibet tninuttinuic werica on s in svct \n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2100: 1.793272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 2200: 1.783671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2300: 1.823992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2400: 1.807575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2500: 1.785991 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2600: 1.775315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 2700: 1.772248 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2800: 1.784275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 2900: 1.758590 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3000: 1.759146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "rian laandar one eight jews an operal english the be two zero six zero s for king\n",
      "ohn one eight feature dischipneight confour zero huis molinmenth tud times radsti\n",
      "ble tin the univers for ideleed was largest as evolused ii the phiction with four\n",
      "qi dig one creatoraining techniques the niever this caesion puming john bemore de\n",
      "png frances weied scuoped a hopkay ball from child was dational overbity of in el\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3100: 1.786546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3200: 1.788323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3300: 1.774099 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 3400: 1.769626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3500: 1.767080 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3600: 1.733614 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 3700: 1.754056 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 3800: 1.765723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3900: 1.770352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 4000: 1.760599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "zqoan olled frecdid was jeed biology has unnwpding time to in or many hurdly bely\n",
      "jigihausb i of like of nametill the be and uppeaa spin folnt of edition film used\n",
      "ng known dounce knowley elfrom siane reservation makerria be since ology belield \n",
      "xxo quet a subsone pulro soman forbets to repubell apprion the proposes sursence \n",
      "c exssorlsits of john diblic mer rubovuls two sols chasperson weike exclees of cu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4100: 1.774000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 4200: 1.754962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4300: 1.749896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 4400: 1.751201 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4500: 1.758852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4600: 1.749652 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4700: 1.742537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 4800: 1.757512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4900: 1.750865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 5000: 1.765087 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "kland han of if jamous to rophauhe menant had alearangehlen violent a tuh two zer\n",
      "ozdant debyron trients tys a hich ausually wast ottent a mean form an one nine tw\n",
      "ave a nature and montswedet medy wimplica x kills centanicko for chita laggetimet\n",
      "ngoines when the adqotions austrian cogramhower new of the ribut of the scounch h\n",
      " refering not is object enge hlearlewed which see the evensus from is telection n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5100: 1.742820 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 5200: 1.739253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5300: 1.729149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5400: 1.712627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5500: 1.714441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5600: 1.725740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5700: 1.722588 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5800: 1.706480 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5900: 1.721393 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 6000: 1.719998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "ip e exilemances and be under and wittif naturabla is callements roinflag eupreme\n",
      "hcinian for storal five one nine three de muian and burrelled mad prce it reaser \n",
      "jbses sopen from sairly into lion the mile majoracciane thoughdy bailled also amg\n",
      "ction mased in but due of chricles de elatern the nah charly instruselation the s\n",
      "mt te their two in filtimonware jultary honual agence of convention of aguean in \n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6100: 1.745124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 6200: 1.721368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 6300: 1.720843 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6400: 1.750236 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6500: 1.762947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6600: 1.733145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 6700: 1.728458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6800: 1.713614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6900: 1.685441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 7000: 1.727475 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "vs peoples prohibilitive start which of the e serves ws in docues leag to as infe\n",
      "brockage furigallger of negrationere the marchilipassuml asia significcent trie a\n",
      "vcal mofs war lisnown an austrained his word it chropose fhenre love programs spe\n",
      "hs alls fortished all disvcyclely cto both late lay most six war us the orphan go\n",
      "tuhapted nine th ancer sfunve bubo one two njectrial on recage the with it is the\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 7100: 1.717526 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 7200: 1.714710 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7300: 1.727509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 7400: 1.719264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 7500: 1.713755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 7600: 1.703736 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 7700: 1.720188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7800: 1.725450 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 7900: 1.733455 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 8000: 1.721249 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "lic one three five classate been battlimits would two dave alread and assal very \n",
      "kuable place from by jurir their tational cyms other countryroon skisn of compute\n",
      "tdeither the phase are united thigneave war is biun the nearly new to and colageh\n",
      "ake near hovered achiejr merinationennive as at has leat influiting today german \n",
      "zrdessinharyounbissions asses son betdbar neynist event and imjlebecounts or nber\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 8100: 1.701923 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 8200: 1.707591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8300: 1.727303 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8400: 1.710439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 8500: 1.729996 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8600: 1.728861 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 8700: 1.722712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 8800: 1.731097 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 8900: 1.704044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 9000: 1.715909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "ydning of if force not a nated and milism haprfn of a one diffne plance stega a f\n",
      "wq viology kome dotactructions with the from life two postical to gain electroyed\n",
      "ddhey isued a famiload election dread black lomen colbogiv it not as associate da\n",
      "ok rings there are portages dnigmt or ellevastras young amedint and the fortent k\n",
      "lgends inatry simply than a larger proteylay three from fit a the eaptize rives t\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.79\n",
      "Average loss at step 9100: 1.723100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 9200: 1.741991 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 9300: 1.730902 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 9400: 1.715412 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 9500: 1.723654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 9600: 1.720761 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 9700: 1.732663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 9800: 1.719707 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 9900: 1.698873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 10000: 1.713460 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "nx internation monstotic monith one old founders uning madwebritish bullet thikor\n",
      "bhasewhen the five yeartry dams populard attation for the eye to seven g was base\n",
      "uith the asia reark is x was and aid governmentarxords rivationals houseading pre\n",
      "im on the halm to strike means chaper happed to the bosting he court mising and h\n",
      "vbilw writz the often but river many micked efilm as law systems physia publish t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 10100: 1.730289 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 10200: 1.724359 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 10300: 1.722170 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 10400: 1.734296 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 10500: 1.745954 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 10600: 1.697134 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 10700: 1.701690 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.95\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 10800: 1.730309 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 10900: 1.737678 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 11000: 1.698670 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "kp pairrological list and screpooe queen banjosext summeution and the aske game i\n",
      "nlable valysis skinformated s beircoasths be dleye senstal of incomputer up revei\n",
      "bdonion jfnhave during what two zero eight feigin those the grabily was an in the\n",
      "ya wi gularl davall anaid the his recodavas of the para is and practicity bpalism\n",
      "ake arus of three recomext in the freedobia day cubtransor in sovis from the not \n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 11100: 1.694475 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 11200: 1.686303 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 11300: 1.687041 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 11400: 1.693311 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 11500: 1.695443 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 11600: 1.673527 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 11700: 1.666083 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 11800: 1.697412 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 11900: 1.680064 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 12000: 1.671008 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "ml or his writer culturever language coids of cade part sunwyrighs penchonal show\n",
      "cculation in tv negative nymphordari in deavocause of new yourg five zero five on\n",
      "gvs child cisioned and posit glamisalossgil bished syster gevious the jaracis km \n",
      "mme or ensed to governmentalist and interrible call four one or mack on a modelk \n",
      "eum temperation of imanies in eject s a mew ross waraved this verred object quart\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 12100: 1.670154 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 12200: 1.687804 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 12300: 1.683530 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 12400: 1.717988 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 12500: 1.702568 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 12600: 1.679918 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 12700: 1.686822 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 12800: 1.697263 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 12900: 1.726571 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 13000: 1.702830 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "qws way thae usit that dates summer bonom of mounts to graphing majordinated the \n",
      "ioning intermber seven four one right lorved storisbols for exchich people insign\n",
      "ectizis this and tishasia the twentile banwith implices island and system an ihod\n",
      "bujce of the beniah description after anism deeptly with the on conference have c\n",
      "cdto colmts transling evict followishes tivers donsuremition of have of tangpsam \n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 13100: 1.699207 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 13200: 1.739086 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 13300: 1.716911 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 13400: 1.721985 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 13500: 1.730505 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 13600: 1.721029 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 13700: 1.691231 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 13800: 1.671257 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 13900: 1.696473 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14000: 1.704779 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.26\n",
      "================================================================================\n",
      "nway shes workisi facum tiversity often indolvate feature exme of being in the te\n",
      "achive gashakang agansline of it is lesslands the substandropsycle are concept im\n",
      "xy one colordents parked it seen were attacks battips pectro modic lor was relart\n",
      "jbs producted in music literator their his regulary whom chall is before deogle c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creator saxony calebrunitive for sealist foars of the two five nine one seven zer\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14100: 1.715770 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 14200: 1.710688 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14300: 1.704007 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14400: 1.718882 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 14500: 1.743735 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 14600: 1.725375 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 14700: 1.733443 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 14800: 1.721854 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 14900: 1.715592 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 15000: 1.712565 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "jdames a cause from the definymns it is since mill and bufy smale sincite this ga\n",
      "xpandle without ravations and lagzy amer that termer ta submistics is ofter futur\n",
      " the aucbh a p athe this priove calleke gated on confirs exemb bresses kitive ass\n",
      "qalse of unn born internaly ariet natably nine earceid toan ret at the posit is h\n",
      "gx of the the anmeancus states of soput is b one the four four the six one eight \n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 15001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2) # Need at least 3 letters for bigram model\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        feed_dict[drop_rate] = 0.10\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2) # Used as buffer\n",
    "                    # Add 2 sample letters to the buffer for input for the bigram model\n",
    "                    feed.extend([random_distribution(), random_distribution()])\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    \n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                             sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[-1])[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                      sample_input[1]: b[1],\n",
    "                                                      drop_rate: 0.0})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15360000  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "batch_size = 128\n",
    "rnn_size = 50\n",
    "num_layers = 2\n",
    "embedding_size = 64\n",
    "learning_rate = 0.001\n",
    "sequence_length = 7\n",
    "\n",
    "end = batch_size*6*20000 # to get ~20k batches with the assumption that average word length should be around 6\n",
    "train_text = text[:end]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example raw data:\n",
      "['', 'anarchism', 'originated', 'as', 'a']\n",
      "Example sequence\n",
      "[[], [0, 13, 0, 17, 2, 7, 8, 18, 12], [14, 17, 8, 6, 8, 13, 0, 19, 4, 3], [0, 18], [0]]\n",
      "\n",
      "\n",
      "Example output\n",
      "[[], [12, 18, 8, 7, 2, 17, 0, 13, 0], [3, 4, 19, 0, 13, 8, 6, 8, 17, 14], [18, 0], [0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dictionaries(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<END>']\n",
    "\n",
    "    words = list(string.ascii_lowercase) + special_words\n",
    "    dictionary = {word: word_i for word_i, word in enumerate(words)}\n",
    "\n",
    "    return dictionary, dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "# build dictionary\n",
    "dictionary, reverse_dictionary = create_dictionaries(train_text)\n",
    "\n",
    "# Convert each word to dictionary representations\n",
    "x_ids = [[dictionary.get(letter, dictionary['<UNK>']) for letter in word] for word in train_text.split(' ')]\n",
    "y_ids = [x[::-1] for x in x_ids]\n",
    "\n",
    "print(\"Example raw data:\")\n",
    "print(train_text.split(' ')[:5])\n",
    "print(\"Example sequence\")\n",
    "print(x_ids[:5])\n",
    "print(\"\\n\")\n",
    "print(\"Example output\")\n",
    "print(y_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell(rnn_size):\n",
    "    cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "    \n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    # Tensors with name are needed to be accessed later when the model is retrieved\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, None], name = 'inputs')\n",
    "    labels = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "\n",
    "    input_sequence_length = tf.placeholder(tf.int32, (batch_size,), name = 'input_sequence_length')\n",
    "    label_sequence_length = tf.placeholder(tf.int32, (batch_size,), name = 'label_sequence_length')\n",
    "    \n",
    "    max_label_sequence_length = tf.reduce_max(label_sequence_length)\n",
    "    \n",
    "    return inputs, labels, lr, input_sequence_length, label_sequence_length, max_label_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(inputs, rnn_size, num_layers,\n",
    "                   input_sequence_length, vocab_size, \n",
    "                   embedding_size):\n",
    "\n",
    "    # embedd the inpiut\n",
    "    embed_input = tf.contrib.layers.embed_sequence(inputs, vocab_size, embedding_size)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(encoder_cell, embed_input, sequence_length=input_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing to remove the last char and add the GO symbol for the decoder\n",
    "def process_decoder_input(labels, dictionary, batch_size):\n",
    "    ending = tf.strided_slice(labels, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], dictionary['<GO>']), ending], 1)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dictionary, embedding_size, num_layers, rnn_size,\n",
    "                   labels_sequence_length, max_label_sequence_length, encoder_state, decoder_input):\n",
    "\n",
    "    vocab_size = len(dictionary)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                            sequence_length=labels_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                           training_helper,\n",
    "                                                           encoder_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,\n",
    "                                                                       maximum_iterations=max_label_sequence_length)[0]\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([dictionary['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                dictionary['<END>'])\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        encoder_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_label_sequence_length)[0]\n",
    "         \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, labels, lr, inputs_sequence_length, labels_sequence_length, \n",
    "                  max_label_sequence_length, vocab_size, embedding_size, rnn_size, num_layers):\n",
    "    \n",
    "    _, encoder_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  inputs_sequence_length,\n",
    "                                  vocab_size, \n",
    "                                  embedding_size)\n",
    "    \n",
    "    decoder_input = process_decoder_input(labels, dictionary, batch_size)\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(dictionary, \n",
    "                                                                       embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       labels_sequence_length,\n",
    "                                                                       max_label_sequence_length,\n",
    "                                                                       encoder_state, \n",
    "                                                                       decoder_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-ab690fd1a1d1>:2: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-22-6c979b9be56b>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-22-6c979b9be56b>:8: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, labels, lr, input_sequence_length, label_sequence_length, max_label_sequence_length = get_model_inputs()\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      labels, \n",
    "                                                                      lr, \n",
    "                                                                      input_sequence_length,\n",
    "                                                                      label_sequence_length, \n",
    "                                                                      max_label_sequence_length, \n",
    "                                                                      len(dictionary),\n",
    "                                                                      embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(label_sequence_length, max_label_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            labels,\n",
    "            masks)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch, pad_int):\n",
    "    # make sure each word has the same length\n",
    "    max_length = max([len(word) for word in batch])\n",
    "    return [word + [pad_int] * (max_length - len(word)) for word in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificiation of the batch generator above, this one iterates the text and returns padded\n",
    "# word representations based on the longest word\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, labels, inputs, batch_size, input_pad_int, label_pad_int):\n",
    "        self._labels = labels\n",
    "        self._inputs = inputs\n",
    "        self._batch_size = batch_size\n",
    "        self._input_pad_int = input_pad_int\n",
    "        self._label_pad_int = label_pad_int\n",
    "        self._cursor = 0\n",
    "  \n",
    "    def next(self):\n",
    "        # Generates a single batch\n",
    "        start_i = self._cursor * self._batch_size\n",
    "        input_batch = self._inputs[start_i:start_i + self._batch_size]\n",
    "        label_batch = self._labels[start_i:start_i + self._batch_size]\n",
    "        pad_input_batch = np.array(pad_batch(input_batch, self._input_pad_int))\n",
    "        pad_label_batch = np.array(pad_batch(label_batch, self._label_pad_int))\n",
    "        \n",
    "        pad_label_lengths = []\n",
    "        for i in pad_label_batch:\n",
    "            pad_label_lengths.append(len(i))\n",
    "        \n",
    "        pad_input_lengths = []\n",
    "        for i in pad_input_batch:\n",
    "            pad_input_lengths.append(len(i))\n",
    "            \n",
    "        self._cursor = self._cursor + 1\n",
    "        \n",
    "        return pad_label_batch, pad_input_batch, pad_label_lengths, pad_input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/20435 - Loss: 0.800  - Validation loss: 0.752\n",
      "Batch 2000/20435 - Loss: 0.319  - Validation loss: 0.362\n",
      "Batch 3000/20435 - Loss: 0.175  - Validation loss: 0.130\n",
      "Batch 4000/20435 - Loss: 0.059  - Validation loss: 0.061\n",
      "Batch 5000/20435 - Loss: 0.022  - Validation loss: 0.030\n",
      "Batch 6000/20435 - Loss: 0.021  - Validation loss: 0.039\n",
      "Batch 7000/20435 - Loss: 0.021  - Validation loss: 0.017\n",
      "Batch 8000/20435 - Loss: 0.009  - Validation loss: 0.017\n",
      "Batch 9000/20435 - Loss: 0.013  - Validation loss: 0.014\n",
      "Batch 10000/20435 - Loss: 0.013  - Validation loss: 0.006\n",
      "Batch 11000/20435 - Loss: 0.009  - Validation loss: 0.007\n",
      "Batch 12000/20435 - Loss: 0.005  - Validation loss: 0.006\n",
      "Batch 13000/20435 - Loss: 0.010  - Validation loss: 0.013\n",
      "Batch 14000/20435 - Loss: 0.003  - Validation loss: 0.004\n",
      "Batch 15000/20435 - Loss: 0.003  - Validation loss: 0.003\n",
      "Batch 16000/20435 - Loss: 0.005  - Validation loss: 0.007\n",
      "Batch 17000/20435 - Loss: 0.001  - Validation loss: 0.002\n",
      "Batch 18000/20435 - Loss: 0.004  - Validation loss: 0.002\n",
      "Batch 19000/20435 - Loss: 0.002  - Validation loss: 0.002\n",
      "Batch 20000/20435 - Loss: 0.011  - Validation loss: 0.014\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Train and validation data split\n",
    "\n",
    "train_input = x_ids[batch_size:]\n",
    "train_labels = y_ids[batch_size:]\n",
    "\n",
    "valid_input = x_ids[:batch_size]\n",
    "valid_labels = y_ids[:batch_size]\n",
    "\n",
    "batch_generator_valid = \\\n",
    "        BatchGenerator(valid_labels, valid_input, batch_size, dictionary['<PAD>'], dictionary['<PAD>'])\n",
    "batch_generator_train = \\\n",
    "        BatchGenerator(train_labels, train_input, batch_size, dictionary['<PAD>'], dictionary['<PAD>'])\n",
    "\n",
    "\n",
    "(valid_labels_batch, valid_input_batch, valid_labels_lengths, valid_input_lengths) = batch_generator_valid.next()\n",
    "\n",
    "display_step = 1000\n",
    "\n",
    "checkpoint = \"seq2seq.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for batch_i in range(0, len(train_input)//batch_size):\n",
    "        \n",
    "        (labels_batch, input_batch, labels_lengths, input_lengths) = batch_generator_train.next()\n",
    "\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: input_batch,\n",
    "             labels: labels_batch,\n",
    "             lr: learning_rate,\n",
    "             label_sequence_length: labels_lengths,\n",
    "             input_sequence_length: input_lengths})\n",
    "\n",
    "        if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "            validation_loss = sess.run(\n",
    "            [cost],\n",
    "            {input_data: valid_input_batch,\n",
    "             labels: valid_labels_batch,\n",
    "             lr: learning_rate,\n",
    "             label_sequence_length: valid_labels_lengths,\n",
    "             input_sequence_length: valid_input_lengths})\n",
    "\n",
    "            print('Batch %d/%d - Loss: %.3f  - Validation loss: %.3f' % (batch_i, \n",
    "                          len(train_input) // batch_size, \n",
    "                          loss, \n",
    "                          validation_loss[0]))\n",
    "\n",
    "    # save the model state\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seq(text):\n",
    "    # preprocess text for the model\n",
    "    return [dictionary.get(word, dictionary['<UNK>']) for word in text] + [dictionary['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angel/Desktop/deep-learning/assignment3/assignment-3/.assign3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./seq2seq.ckpt\n",
      "\n",
      "Original Text: the quick brown fox\n",
      "  Word Ids: [[19, 7, 4, 26, 26, 26, 26], [16, 20, 8, 2, 10, 26, 26], [1, 17, 14, 22, 13, 26, 26], [5, 14, 23, 26, 26, 26, 26]]\n",
      "Input words: ['t h e <PAD> <PAD> <PAD> <PAD>', 'q u i c k <PAD> <PAD>', 'b r o w n <PAD> <PAD>', 'f o x <PAD> <PAD> <PAD> <PAD>']\n",
      "\n",
      "Output:\n",
      "Word Ids: [4, 7, 19]\n",
      "Response Word: eht\n",
      "Word Ids: [10, 2, 8, 20, 16]\n",
      "Response Word: kciuq\n",
      "Word Ids: [13, 22, 14, 17, 1]\n",
      "Response Word: nworb\n",
      "Word Ids: [23, 14, 5]\n",
      "Response Word: xof\n",
      "\n",
      "Sentence output: eht kciuq nworb xof\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence + output\n",
    "# The sentence is split into words and fed to the seq2seq model, then each word is outputed + the final sentence\n",
    "# at the end.\n",
    "\n",
    "pad = dictionary[\"<PAD>\"] \n",
    "\n",
    "checkpoint = \"./seq2seq.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    \n",
    "    # load the model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_sequence_length = loaded_graph.get_tensor_by_name('input_sequence_length:0')\n",
    "    label_sequence_length = loaded_graph.get_tensor_by_name('label_sequence_length:0')\n",
    "    \n",
    "    input_sentence = 'the quick brown fox'\n",
    "    words = input_sentence.split(' ')\n",
    "    \n",
    "    print()\n",
    "    print('Original Text: %s' % input_sentence)\n",
    "    print('  Word Ids: %s' % ([[letter for letter in to_seq(word)] for word in words]))\n",
    "    inputWords = [\" \".join([reverse_dictionary[i] for i in to_seq(word)]) for word in words]\n",
    "    print('Input words: %s' % inputWords)\n",
    "    \n",
    "    print()\n",
    "    print(\"Output:\")\n",
    "    outputs = list()\n",
    "    for word in input_sentence.split(' '):\n",
    "        word_seq = to_seq(word)\n",
    "        answer_logits = sess.run(logits, {input_data: [word_seq]*batch_size, \n",
    "                                      label_sequence_length: [len(word_seq)]*batch_size, \n",
    "                                      input_sequence_length: [len(word_seq)]*batch_size})[0]\n",
    "\n",
    "        \n",
    "        output_word = ''.join([reverse_dictionary[i] for i in answer_logits if i != pad])\n",
    "\n",
    "        print('Word Ids: %s' % ([i for i in answer_logits if i != pad]))\n",
    "        print('Response Word: %s' % (output_word))\n",
    "        outputs.append(output_word)\n",
    "    \n",
    "    print()\n",
    "    print(\"Sentence output: %s\" % (' '.join(outputs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
